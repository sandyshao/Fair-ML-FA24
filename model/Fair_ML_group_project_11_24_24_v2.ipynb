{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair GAN Implementation for Adult Census Dataset\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:19:22.887913Z",
     "start_time": "2024-11-24T05:19:21.162478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ucimlrepo) (2.1.4)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ucimlrepo) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: 2.4.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting aif360\n",
      "  Downloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aif360) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aif360) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aif360) (1.3.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aif360) (3.8.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aif360) (1.11.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=0.24.0->aif360) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=0.24.0->aif360) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=1.0->aif360) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=1.0->aif360) (1.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (1.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (10.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (4.47.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (3.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->aif360) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360) (1.16.0)\n",
      "Installing collected packages: aif360\n",
      "Successfully installed aif360-0.6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo\n",
    "%pip install tensorflow>=2.4.0\n",
    "%pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:19:22.893914Z",
     "start_time": "2024-11-24T05:19:22.889770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import ssl\n",
    "import warnings\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:19:24.241242Z",
     "start_time": "2024-11-24T05:19:22.894601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (48842, 15)\n",
      "\n",
      "Columns: ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare dataset\n",
    "adult = fetch_ucirepo(id=2)\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "y.columns = ['income']\n",
    "full_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "print(\"Dataset shape:\", full_data.shape)\n",
    "print(\"\\nColumns:\", full_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:19:24.291193Z",
     "start_time": "2024-11-24T05:19:24.242478Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    categorical_features = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                          'relationship', 'race', 'sex', 'native-country']\n",
    "    \n",
    "    numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain',\n",
    "                         'capital-loss', 'hours-per-week']\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for feature in categorical_features:\n",
    "        label_encoders[feature] = LabelEncoder()\n",
    "        df_copy[feature] = label_encoders[feature].fit_transform(df_copy[feature].astype(str))\n",
    "    \n",
    "    df_copy['income'] = df_copy['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_copy[numerical_features] = scaler.fit_transform(df_copy[numerical_features])\n",
    "    \n",
    "    return df_copy, label_encoders, scaler\n",
    "\n",
    "processed_data, label_encoders, scaler = preprocess_data(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:19:24.307643Z",
     "start_time": "2024-11-24T05:19:24.291818Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAN_with_FairnessDebiasing(tf.keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim=100):\n",
    "        super(GAN_with_FairnessDebiasing, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.last_d_loss = 1.0\n",
    "        \n",
    "        # Generator with conditioning\n",
    "        # size of the input can be adjusted \n",
    "        self.generator = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, input_dim=latent_dim + 2),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            \n",
    "            tf.keras.layers.Dense(512),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            \n",
    "            tf.keras.layers.Dense(input_dim),\n",
    "            tf.keras.layers.Activation('sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Main discriminator\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_dim=input_dim),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            \n",
    "            tf.keras.layers.Dense(64),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            \n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Fairness discriminator\n",
    "        self.fairness_discriminator = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_dim=input_dim),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Dense(2, activation='sigmoid')  # Changed to output 2 values for sex and race\n",
    "        ])\n",
    "        \n",
    "        # Learning rate can be adjusted\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=00.00001, beta_1=0.5)\n",
    "        self.f_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    def demographic_parity_loss(self, data):\n",
    "        # Extract protected features\n",
    "        sex = data[:, 9:10]  \n",
    "        race = data[:, 8:9]  \n",
    "        protected_features = tf.concat([sex, race], axis=1)\n",
    "        \n",
    "        predictions = self.discriminator(data)\n",
    "        \n",
    "        # Calculate means for different demographic groups\n",
    "        priv_mask = tf.reduce_all(tf.greater(protected_features, 0.5), axis=1)\n",
    "        unpriv_mask = tf.reduce_all(tf.less_equal(protected_features, 0.5), axis=1)\n",
    "        \n",
    "        # Ensure mask has correct shape\n",
    "        priv_mask = tf.reshape(priv_mask, [-1])\n",
    "        unpriv_mask = tf.reshape(unpriv_mask, [-1])\n",
    "        \n",
    "        # Safe handling of empty groups\n",
    "        priv_preds = tf.boolean_mask(predictions, priv_mask)\n",
    "        unpriv_preds = tf.boolean_mask(predictions, unpriv_mask)\n",
    "        \n",
    "        priv_mean = tf.reduce_mean(priv_preds) if tf.size(priv_preds) > 0 else 0.0\n",
    "        unpriv_mean = tf.reduce_mean(unpriv_preds) if tf.size(unpriv_preds) > 0 else 0.0\n",
    "        \n",
    "        return tf.abs(priv_mean - unpriv_mean)\n",
    "\n",
    "    def equalized_odds_loss(self, data, labels):\n",
    "        # Extract protected features\n",
    "        sex = data[:, 9:10]  # sex column\n",
    "        race = data[:, 8:9]  # race column\n",
    "        protected_features = tf.concat([sex, race], axis=1)\n",
    "        \n",
    "        predictions = self.discriminator(data)\n",
    "        \n",
    "        # Ensure labels have correct shape\n",
    "        labels = tf.cast(tf.reshape(labels, [-1, 1]), tf.float32)\n",
    "        \n",
    "        # Calculate true positive rates for different groups\n",
    "        priv_mask = tf.reduce_all(tf.greater(protected_features, 0.5), axis=1)\n",
    "        unpriv_mask = tf.reduce_all(tf.less_equal(protected_features, 0.5), axis=1)\n",
    "        positive_mask = tf.reshape(tf.greater(labels, 0.5), [-1])\n",
    "        \n",
    "        # Safe handling of group intersections\n",
    "        priv_pos_mask = tf.logical_and(priv_mask, positive_mask)\n",
    "        unpriv_pos_mask = tf.logical_and(unpriv_mask, positive_mask)\n",
    "        \n",
    "        priv_pos_preds = tf.boolean_mask(predictions, priv_pos_mask)\n",
    "        unpriv_pos_preds = tf.boolean_mask(predictions, unpriv_pos_mask)\n",
    "        \n",
    "        priv_tpr = tf.reduce_mean(priv_pos_preds) if tf.size(priv_pos_preds) > 0 else 0.0\n",
    "        unpriv_tpr = tf.reduce_mean(unpriv_pos_preds) if tf.size(unpriv_pos_preds) > 0 else 0.0\n",
    "        \n",
    "        return tf.abs(priv_tpr - unpriv_tpr)\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        \n",
    "        # Extract labels and protected attributes\n",
    "        labels = real_data[:, -1]  # income is the last column\n",
    "        protected_features = tf.concat([\n",
    "            real_data[:, 9:10],\n",
    "            real_data[:, 8:9]    \n",
    "        ], axis=1)\n",
    "        \n",
    "        # Generate balanced conditions\n",
    "        sex = tf.random.uniform([batch_size, 1], 0.4, 0.6) > 0.5\n",
    "        race = tf.random.uniform([batch_size, 1], 0.4, 0.6) > 0.5\n",
    "        conditions = tf.concat([tf.cast(sex, tf.float32), \n",
    "                              tf.cast(race, tf.float32)], axis=1)\n",
    "        \n",
    "        # Train Generator\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        conditional_noise = tf.concat([noise, conditions], axis=1)\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            generated_data = self.generator(conditional_noise, training=True)\n",
    "            fake_output = self.discriminator(generated_data, training=True)\n",
    "            \n",
    "            gen_loss = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "            dp_loss = self.demographic_parity_loss(generated_data)\n",
    "            eo_loss = self.equalized_odds_loss(generated_data, labels)\n",
    "            \n",
    "            fairness_weight = 0.5\n",
    "            total_gen_loss = gen_loss + fairness_weight * (dp_loss + eo_loss)\n",
    "        \n",
    "        gen_grads = gen_tape.gradient(total_gen_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        \n",
    "        # Train Discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(conditional_noise, training=True)\n",
    "            real_output = self.discriminator(real_data, training=True)\n",
    "            fake_output = self.discriminator(generated_data, training=True)\n",
    "            \n",
    "            # These can be adjusted \n",
    "            real_labels = tf.random.uniform([batch_size, 1], 0.8, 0.9)\n",
    "            fake_labels = tf.random.uniform([batch_size, 1], 0.1, 0.2)\n",
    "            \n",
    "            real_loss = self.loss_fn(real_labels, real_output)\n",
    "            fake_loss = self.loss_fn(fake_labels, fake_output)\n",
    "            disc_loss = (real_loss + fake_loss) / 2\n",
    "            \n",
    "            dp_reg = self.demographic_parity_loss(real_data)\n",
    "            eo_reg = self.equalized_odds_loss(real_data, labels)\n",
    "            total_disc_loss = disc_loss + fairness_weight * (dp_reg + eo_reg)\n",
    "        \n",
    "        if disc_loss < 1.0:\n",
    "            disc_grads = disc_tape.gradient(total_disc_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train Fairness Discriminator\n",
    "        with tf.GradientTape() as fair_tape:\n",
    "            fair_pred_real = self.fairness_discriminator(real_data, training=True)\n",
    "            fair_pred_fake = self.fairness_discriminator(generated_data, training=True)\n",
    "            \n",
    "            fair_loss = (self.loss_fn(protected_features, fair_pred_real) + \n",
    "                        self.loss_fn(conditions, fair_pred_fake)) / 2\n",
    "        \n",
    "        fair_grads = fair_tape.gradient(fair_loss, self.fairness_discriminator.trainable_variables)\n",
    "        self.f_optimizer.apply_gradients(zip(fair_grads, self.fairness_discriminator.trainable_variables))\n",
    "        \n",
    "        return float(total_disc_loss.numpy()), float(total_gen_loss.numpy())\n",
    "\n",
    "    def train(self, dataset, epochs=10, batch_size=512):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(1000).batch(batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_d_loss = 0.0\n",
    "            total_g_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch in dataset:\n",
    "                d_loss, g_loss = self.train_step(batch)\n",
    "                total_d_loss += d_loss\n",
    "                total_g_loss += g_loss\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_d_loss = total_d_loss / num_batches\n",
    "            avg_g_loss = total_g_loss / num_batches\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n",
    "            \n",
    "    def generate_samples(self, num_samples):\n",
    "        # Generate balanced conditions\n",
    "        sex = np.random.binomial(1, 0.5, (num_samples, 1))\n",
    "        race = np.random.binomial(1, 0.5, (num_samples, 1))\n",
    "        conditions = np.concatenate([sex, race], axis=1)\n",
    "        \n",
    "        # Generate samples with conditions\n",
    "        noise = tf.random.normal([num_samples, self.latent_dim])\n",
    "        conditional_noise = tf.concat([noise, tf.convert_to_tensor(conditions, dtype=tf.float32)], axis=1)\n",
    "        generated = self.generator(conditional_noise).numpy()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        generated_df = pd.DataFrame(generated, columns=processed_data.columns)\n",
    "        \n",
    "        # Ensure binary values\n",
    "        binary_features = ['income', 'sex', 'race']\n",
    "        for feature in binary_features:\n",
    "            generated_df[feature] = (generated_df[feature] > 0.5).astype(float)\n",
    "        \n",
    "        return generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:26:49.758044Z",
     "start_time": "2024-11-24T05:26:30.454594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/20, D Loss: 1.3118, G Loss: 0.6600\n",
      "Epoch 2/20, D Loss: 1.3457, G Loss: 0.6582\n",
      "Epoch 3/20, D Loss: 1.3000, G Loss: 0.6564\n",
      "Epoch 4/20, D Loss: 1.3535, G Loss: 0.6547\n",
      "Epoch 5/20, D Loss: 1.3533, G Loss: 0.6534\n",
      "Epoch 6/20, D Loss: 1.3596, G Loss: 0.6522\n",
      "Epoch 7/20, D Loss: 1.3039, G Loss: 0.6506\n",
      "Epoch 8/20, D Loss: 1.4044, G Loss: 0.6491\n",
      "Epoch 9/20, D Loss: 1.3593, G Loss: 0.6479\n",
      "Epoch 10/20, D Loss: 1.3089, G Loss: 0.6465\n",
      "Epoch 11/20, D Loss: 1.3661, G Loss: 0.6460\n",
      "Epoch 12/20, D Loss: 1.3144, G Loss: 0.6444\n",
      "Epoch 13/20, D Loss: 1.3632, G Loss: 0.6444\n",
      "Epoch 14/20, D Loss: 1.3089, G Loss: 0.6435\n",
      "Epoch 15/20, D Loss: 1.3519, G Loss: 0.6427\n",
      "Epoch 16/20, D Loss: 1.3577, G Loss: 0.6416\n",
      "Epoch 17/20, D Loss: 1.3597, G Loss: 0.6419\n",
      "Epoch 18/20, D Loss: 1.3089, G Loss: 0.6397\n",
      "Epoch 19/20, D Loss: 1.3560, G Loss: 0.6389\n",
      "Epoch 20/20, D Loss: 1.3607, G Loss: 0.6386\n",
      "        age  workclass    fnlwgt  education  education-num  marital-status  \\\n",
      "0  0.508776   0.526309  0.734007   0.525169       0.299421        0.538781   \n",
      "1  0.403153   0.836837  0.509085   0.387007       0.598736        0.462221   \n",
      "2  0.318674   0.507776  0.822090   0.676006       0.127619        0.894895   \n",
      "3  0.184091   0.868463  0.614778   0.206727       0.553461        0.656708   \n",
      "4  0.843896   0.643885  0.471465   0.582001       0.317607        0.545549   \n",
      "\n",
      "   occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "0    0.411906      0.669505   1.0  0.0      0.512545      0.536994   \n",
      "1    0.628681      0.410298   0.0  0.0      0.449886      0.384628   \n",
      "2    0.393811      0.541384   1.0  0.0      0.520662      0.180286   \n",
      "3    0.683295      0.544430   0.0  1.0      0.781951      0.554960   \n",
      "4    0.713641      0.481608   0.0  1.0      0.621369      0.089298   \n",
      "\n",
      "   hours-per-week  native-country  income  \n",
      "0        0.626136        0.326551     1.0  \n",
      "1        0.492885        0.540337     0.0  \n",
      "2        0.793848        0.571747     1.0  \n",
      "3        0.785856        0.671492     0.0  \n",
      "4        0.676745        0.901763     1.0  \n"
     ]
    }
   ],
   "source": [
    "# Initialize and train FairGAN\n",
    "gan = GAN_with_FairnessDebiasing(input_dim=processed_data.shape[1])\n",
    "print(\"Starting training...\")\n",
    "gan.train(processed_data.values, epochs=20, batch_size=10000)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_samples = gan.generate_samples(1000)\n",
    "synthetic_df = pd.DataFrame(synthetic_samples, columns=processed_data.columns)\n",
    "print (synthetic_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T05:26:55.842887Z",
     "start_time": "2024-11-24T05:26:55.790654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Data Metrics:\n",
      "\n",
      "Fairness Metrics for sex:\n",
      "Disparate Impact: 0.357\n",
      "Statistical Parity Difference: -0.131\n",
      "\n",
      "Fairness Metrics for race:\n",
      "Disparate Impact: 0.422\n",
      "Statistical Parity Difference: -0.105\n",
      "\n",
      "Synthetic Data Metrics:\n",
      "\n",
      "Fairness Metrics for sex:\n",
      "Disparate Impact: 0.675\n",
      "Statistical Parity Difference: -0.118\n",
      "\n",
      "Fairness Metrics for race:\n",
      "Disparate Impact: 1.139\n",
      "Statistical Parity Difference: 0.039\n",
      "\n",
      "Synthetic Data Distribution:\n",
      "Income distribution: income\n",
      "0.0    702\n",
      "1.0    298\n",
      "Name: count, dtype: int64\n",
      "Sex distribution: sex\n",
      "0.0    561\n",
      "1.0    439\n",
      "Name: count, dtype: int64\n",
      "Race distribution: race\n",
      "0.0    518\n",
      "1.0    482\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fairness(dataset, protected_attribute):\n",
    "    metrics = BinaryLabelDatasetMetric(\n",
    "        dataset, \n",
    "        unprivileged_groups=[{protected_attribute: 0}],\n",
    "        privileged_groups=[{protected_attribute: 1}]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFairness Metrics for {protected_attribute}:\")\n",
    "    print(f\"Disparate Impact: {metrics.disparate_impact():.3f}\")\n",
    "    print(f\"Statistical Parity Difference: {metrics.statistical_parity_difference():.3f}\")\n",
    "# Create AIF360 dataset with label specifications\n",
    "aif_dataset = BinaryLabelDataset(\n",
    "    df=processed_data,\n",
    "    label_names=['income'],\n",
    "    protected_attribute_names=['sex', 'race'],\n",
    "    privileged_protected_attributes=[1, 1],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "# Print original metrics\n",
    "print(\"\\nOriginal Data Metrics:\")\n",
    "evaluate_fairness(aif_dataset, 'sex')\n",
    "evaluate_fairness(aif_dataset, 'race')\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_samples = gan.generate_samples(1000)\n",
    "synthetic_df = pd.DataFrame(synthetic_samples, columns=processed_data.columns)\n",
    "\n",
    "# Round the synthetic data to ensure binary values\n",
    "synthetic_df['income'] = synthetic_df['income'].round().clip(0, 1)  # Ensure values are 0 or 1\n",
    "synthetic_df['sex'] = synthetic_df['sex'].round().clip(0, 1)       # Ensure values are 0 or 1\n",
    "synthetic_df['race'] = synthetic_df['race'].round().clip(0, 1)     # Ensure values are 0 or 1\n",
    "\n",
    "# Create synthetic dataset\n",
    "synthetic_aif_dataset = BinaryLabelDataset(\n",
    "    df=synthetic_df,\n",
    "    label_names=['income'],\n",
    "    protected_attribute_names=['sex', 'race'],\n",
    "    privileged_protected_attributes=[1, 1],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "print(\"\\nSynthetic Data Metrics:\")\n",
    "evaluate_fairness(synthetic_aif_dataset, 'sex')\n",
    "evaluate_fairness(synthetic_aif_dataset, 'race')\n",
    "\n",
    "# Optional: Print distribution of values to verify\n",
    "print(\"\\nSynthetic Data Distribution:\")\n",
    "print(\"Income distribution:\", synthetic_df['income'].value_counts())\n",
    "print(\"Sex distribution:\", synthetic_df['sex'].value_counts())\n",
    "print(\"Race distribution:\", synthetic_df['race'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
