{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11864e41-e105-4f29-bff2-9b4c45e3b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow.compat.v1 as tf\n",
    "except ImportError as error:\n",
    "    from logging import warning\n",
    "    warning(\"{}: AdversarialDebiasing will be unavailable. To install, run:\\n\"\n",
    "            \"pip install 'aif360[AdversarialDebiasing]'\".format(error))\n",
    "\n",
    "from aif360.algorithms import Transformer\n",
    "\n",
    "\n",
    "class AdversarialDebiasing(Transformer):\n",
    "    \"\"\"Adversarial debiasing is an in-processing technique that learns a\n",
    "    classifier to maximize prediction accuracy and simultaneously reduce an\n",
    "    adversary's ability to determine the protected attribute from the\n",
    "    predictions [5]_. This approach leads to a fair classifier as the\n",
    "    predictions cannot carry any group discrimination information that the\n",
    "    adversary can exploit.\n",
    "\n",
    "    References:\n",
    "        .. [5] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating Unwanted\n",
    "           Biases with Adversarial Learning,\" AAAI/ACM Conference on Artificial\n",
    "           Intelligence, Ethics, and Society, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unprivileged_groups,\n",
    "                 privileged_groups,\n",
    "                 scope_name,\n",
    "                 sess,\n",
    "                 seed=None,\n",
    "                 adversary_loss_weight=0.1,\n",
    "                 num_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 classifier_num_hidden_units=200,\n",
    "                 debias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (tuple): Representation for unprivileged groups\n",
    "            privileged_groups (tuple): Representation for privileged groups\n",
    "            scope_name (str): scope name for the tenforflow variables\n",
    "            sess (tf.Session): tensorflow session\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "            adversary_loss_weight (float, optional): Hyperparameter that chooses\n",
    "                the strength of the adversarial loss.\n",
    "            num_epochs (int, optional): Number of training epochs.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            classifier_num_hidden_units (int, optional): Number of hidden units\n",
    "                in the classifier model.\n",
    "            debias (bool, optional): Learn a classifier with or without\n",
    "                debiasing.\n",
    "        \"\"\"\n",
    "        super(AdversarialDebiasing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "        if len(self.unprivileged_groups) > 1 or len(self.privileged_groups) > 1:\n",
    "            raise ValueError(\"Only one unprivileged_group or privileged_group supported.\")\n",
    "        self.protected_attribute_name = list(self.unprivileged_groups[0].keys())[0]\n",
    "\n",
    "        self.sess = sess\n",
    "        self.adversary_loss_weight = adversary_loss_weight\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
    "        self.debias = debias\n",
    "\n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def _classifier_model(self, features, features_dim, keep_prob):\n",
    "        \"\"\"Compute the classifier predictions for the outcome variable.\n",
    "\n",
    "        Input\n",
    "        features: a tensor representing the input features for the classifier\n",
    "        features_dim: dimension of the feature dataset\n",
    "\n",
    "        Return\n",
    "        pred_label: predicted label\n",
    "        pred_logits: raw logits output from the output layer that holds\n",
    "        the unactivated score of prediction confidence\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"classifier_model\"):\n",
    "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
    "                                  initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
    "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
    "\n",
    "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_logit = tf.matmul(h1, W2) + b2\n",
    "            pred_label = tf.sigmoid(pred_logit) # predictive binary classification with sigmoid activation\n",
    "\n",
    "        return pred_label, pred_logit\n",
    "\n",
    "    def _adversary_model(self, pred_logits, true_labels):\n",
    "        \"\"\"Compute the adversary predictions for the protected attribute.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [3, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_protected_attribute_logit = tf.matmul(tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1), W2) + b2\n",
    "            pred_protected_attribute_label = tf.sigmoid(pred_protected_attribute_logit)\n",
    "\n",
    "        return pred_protected_attribute_label, pred_protected_attribute_logit\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Compute the model parameters of the fair classifier using gradient\n",
    "        descent.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "\n",
    "        Returns:\n",
    "            AdversarialDebiasing: Returns self.\n",
    "        \"\"\"\n",
    "        if tf.executing_eagerly():\n",
    "            raise RuntimeError(\"AdversarialDebiasing does not work in eager \"\n",
    "                    \"execution mode. To fix, add `tf.disable_eager_execution()`\"\n",
    "                    \" to the top of the calling script.\")\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        ii32 = np.iinfo(np.int32)\n",
    "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(ii32.min, ii32.max, size=4)\n",
    "\n",
    "        # Map the dataset labels to 0 and 1.\n",
    "        temp_labels = dataset.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(),0] = 1.0\n",
    "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(),0] = 0.0\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            # cross entropy loss between true and predicted labels\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(pred_logits, self.true_labels_ph)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                       1000, 0.96, staircase=True)\n",
    "            # Tensorflow optimizer for classifier\n",
    "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                # Tensorflow optimizer for adversary\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            classifier_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'classifier_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'adversary_model' in var.name]\n",
    "                # Update classifier parameters\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n",
    "                                                                                      var_list=classifier_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "            classifier_grads = []\n",
    "            for (grad,var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                classifier_grads.append((grad, var))\n",
    "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                with tf.control_dependencies([classifier_minimizer]):\n",
    "                    adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars)#, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = dataset.features[batch_ids]\n",
    "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                                 dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                       self.true_labels_ph: batch_labels,\n",
    "                                       self.protected_attributes_ph: batch_protected_attributes,\n",
    "                                       self.keep_prob: 0.8}\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([classifier_minimizer,\n",
    "                                       adversary_minimizer,\n",
    "                                       pred_labels_loss,\n",
    "                                       pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run(\n",
    "                            [classifier_minimizer,\n",
    "                             pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair\n",
    "        classifier learned.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "        Returns:\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(dataset.features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = dataset.features[batch_ids]\n",
    "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                         dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy(deepcopy = True)\n",
    "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
    "        dataset_new.labels = (np.array(pred_labels)>0.5).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Map the dataset labels to back to their original values.\n",
    "        temp_labels = dataset_new.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
    "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
    "\n",
    "        dataset_new.labels = temp_labels.copy()\n",
    "\n",
    "        return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daacc6-d746-4b12-b7aa-c695f4720a7a",
   "metadata": {},
   "source": [
    "# Test local AD impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89447823-c5b9-466b-8507-47dbaf264f7c",
   "metadata": {},
   "source": [
    "## Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77feeaee-f888-4a6d-acc5-2ec9c6075194",
   "metadata": {},
   "source": [
    "### sensitive attribute = sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71038c9-d632-43c2-9970-c22bfe750226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Sandy/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Sandy/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 48.314850; batch adversarial loss: 1.134260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733092916.767180  639727 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 200; batch classifier loss: 10.646967; batch adversarial loss: 1.049885\n",
      "epoch 1; iter: 0; batch classifier loss: 8.462988; batch adversarial loss: 0.757050\n",
      "epoch 1; iter: 200; batch classifier loss: 5.221793; batch adversarial loss: 0.823377\n",
      "epoch 2; iter: 0; batch classifier loss: 2.613160; batch adversarial loss: 0.752772\n",
      "epoch 2; iter: 200; batch classifier loss: 4.986633; batch adversarial loss: 0.707846\n",
      "epoch 3; iter: 0; batch classifier loss: 5.351168; batch adversarial loss: 0.688093\n",
      "epoch 3; iter: 200; batch classifier loss: 2.172413; batch adversarial loss: 0.664870\n",
      "epoch 4; iter: 0; batch classifier loss: 1.712467; batch adversarial loss: 0.616332\n",
      "epoch 4; iter: 200; batch classifier loss: 1.110458; batch adversarial loss: 0.625024\n",
      "epoch 5; iter: 0; batch classifier loss: 0.852619; batch adversarial loss: 0.578249\n",
      "epoch 5; iter: 200; batch classifier loss: 0.576080; batch adversarial loss: 0.646356\n",
      "epoch 6; iter: 0; batch classifier loss: 1.557930; batch adversarial loss: 0.640221\n",
      "epoch 6; iter: 200; batch classifier loss: 0.746857; batch adversarial loss: 0.640421\n",
      "epoch 7; iter: 0; batch classifier loss: 1.198074; batch adversarial loss: 0.678730\n",
      "epoch 7; iter: 200; batch classifier loss: 0.571567; batch adversarial loss: 0.654624\n",
      "epoch 8; iter: 0; batch classifier loss: 0.601516; batch adversarial loss: 0.609715\n",
      "epoch 8; iter: 200; batch classifier loss: 0.405702; batch adversarial loss: 0.620137\n",
      "epoch 9; iter: 0; batch classifier loss: 0.595322; batch adversarial loss: 0.609998\n",
      "epoch 9; iter: 200; batch classifier loss: 0.580970; batch adversarial loss: 0.637152\n",
      "epoch 10; iter: 0; batch classifier loss: 0.487276; batch adversarial loss: 0.648509\n",
      "epoch 10; iter: 200; batch classifier loss: 0.338899; batch adversarial loss: 0.629343\n",
      "epoch 11; iter: 0; batch classifier loss: 0.350056; batch adversarial loss: 0.563106\n",
      "epoch 11; iter: 200; batch classifier loss: 0.374812; batch adversarial loss: 0.609130\n",
      "epoch 12; iter: 0; batch classifier loss: 0.401039; batch adversarial loss: 0.634886\n",
      "epoch 12; iter: 200; batch classifier loss: 0.395386; batch adversarial loss: 0.600267\n",
      "epoch 13; iter: 0; batch classifier loss: 0.372854; batch adversarial loss: 0.598762\n",
      "epoch 13; iter: 200; batch classifier loss: 0.543044; batch adversarial loss: 0.612449\n",
      "epoch 14; iter: 0; batch classifier loss: 0.501084; batch adversarial loss: 0.625467\n",
      "epoch 14; iter: 200; batch classifier loss: 0.400182; batch adversarial loss: 0.613020\n",
      "epoch 15; iter: 0; batch classifier loss: 0.622033; batch adversarial loss: 0.639541\n",
      "epoch 15; iter: 200; batch classifier loss: 0.424642; batch adversarial loss: 0.672964\n",
      "epoch 16; iter: 0; batch classifier loss: 0.365013; batch adversarial loss: 0.569588\n",
      "epoch 16; iter: 200; batch classifier loss: 0.418848; batch adversarial loss: 0.609323\n",
      "epoch 17; iter: 0; batch classifier loss: 0.362498; batch adversarial loss: 0.627431\n",
      "epoch 17; iter: 200; batch classifier loss: 0.459670; batch adversarial loss: 0.623106\n",
      "epoch 18; iter: 0; batch classifier loss: 0.426325; batch adversarial loss: 0.621433\n",
      "epoch 18; iter: 200; batch classifier loss: 0.471884; batch adversarial loss: 0.629325\n",
      "epoch 19; iter: 0; batch classifier loss: 0.429050; batch adversarial loss: 0.565156\n",
      "epoch 19; iter: 200; batch classifier loss: 0.534661; batch adversarial loss: 0.580990\n",
      "epoch 20; iter: 0; batch classifier loss: 0.380726; batch adversarial loss: 0.627517\n",
      "epoch 20; iter: 200; batch classifier loss: 0.361115; batch adversarial loss: 0.634588\n",
      "epoch 21; iter: 0; batch classifier loss: 0.384497; batch adversarial loss: 0.559202\n",
      "epoch 21; iter: 200; batch classifier loss: 0.374945; batch adversarial loss: 0.612513\n",
      "epoch 22; iter: 0; batch classifier loss: 0.423483; batch adversarial loss: 0.617644\n",
      "epoch 22; iter: 200; batch classifier loss: 0.292608; batch adversarial loss: 0.660694\n",
      "epoch 23; iter: 0; batch classifier loss: 0.332288; batch adversarial loss: 0.636996\n",
      "epoch 23; iter: 200; batch classifier loss: 0.480683; batch adversarial loss: 0.612080\n",
      "epoch 24; iter: 0; batch classifier loss: 0.333658; batch adversarial loss: 0.677328\n",
      "epoch 24; iter: 200; batch classifier loss: 0.340440; batch adversarial loss: 0.607391\n",
      "epoch 25; iter: 0; batch classifier loss: 0.385499; batch adversarial loss: 0.611394\n",
      "epoch 25; iter: 200; batch classifier loss: 0.398684; batch adversarial loss: 0.535844\n",
      "epoch 26; iter: 0; batch classifier loss: 0.397044; batch adversarial loss: 0.605600\n",
      "epoch 26; iter: 200; batch classifier loss: 0.344291; batch adversarial loss: 0.604988\n",
      "epoch 27; iter: 0; batch classifier loss: 0.337300; batch adversarial loss: 0.635675\n",
      "epoch 27; iter: 200; batch classifier loss: 0.297019; batch adversarial loss: 0.661540\n",
      "epoch 28; iter: 0; batch classifier loss: 0.324038; batch adversarial loss: 0.586085\n",
      "epoch 28; iter: 200; batch classifier loss: 0.263948; batch adversarial loss: 0.622848\n",
      "epoch 29; iter: 0; batch classifier loss: 0.313512; batch adversarial loss: 0.612815\n",
      "epoch 29; iter: 200; batch classifier loss: 0.366802; batch adversarial loss: 0.553731\n",
      "epoch 30; iter: 0; batch classifier loss: 0.394665; batch adversarial loss: 0.631600\n",
      "epoch 30; iter: 200; batch classifier loss: 0.365675; batch adversarial loss: 0.604194\n",
      "epoch 31; iter: 0; batch classifier loss: 0.420337; batch adversarial loss: 0.663199\n",
      "epoch 31; iter: 200; batch classifier loss: 0.323618; batch adversarial loss: 0.621889\n",
      "epoch 32; iter: 0; batch classifier loss: 0.294043; batch adversarial loss: 0.614670\n",
      "epoch 32; iter: 200; batch classifier loss: 0.364143; batch adversarial loss: 0.604638\n",
      "epoch 33; iter: 0; batch classifier loss: 0.347116; batch adversarial loss: 0.660761\n",
      "epoch 33; iter: 200; batch classifier loss: 0.386212; batch adversarial loss: 0.613889\n",
      "epoch 34; iter: 0; batch classifier loss: 0.405549; batch adversarial loss: 0.675028\n",
      "epoch 34; iter: 200; batch classifier loss: 0.312177; batch adversarial loss: 0.605645\n",
      "epoch 35; iter: 0; batch classifier loss: 0.329889; batch adversarial loss: 0.553440\n",
      "epoch 35; iter: 200; batch classifier loss: 0.296043; batch adversarial loss: 0.621281\n",
      "epoch 36; iter: 0; batch classifier loss: 0.331361; batch adversarial loss: 0.603651\n",
      "epoch 36; iter: 200; batch classifier loss: 0.279556; batch adversarial loss: 0.615458\n",
      "epoch 37; iter: 0; batch classifier loss: 0.378815; batch adversarial loss: 0.645769\n",
      "epoch 37; iter: 200; batch classifier loss: 0.466447; batch adversarial loss: 0.644004\n",
      "epoch 38; iter: 0; batch classifier loss: 0.304113; batch adversarial loss: 0.661224\n",
      "epoch 38; iter: 200; batch classifier loss: 0.287013; batch adversarial loss: 0.637390\n",
      "epoch 39; iter: 0; batch classifier loss: 0.425049; batch adversarial loss: 0.599046\n",
      "epoch 39; iter: 200; batch classifier loss: 0.395046; batch adversarial loss: 0.634162\n",
      "epoch 40; iter: 0; batch classifier loss: 0.345297; batch adversarial loss: 0.579250\n",
      "epoch 40; iter: 200; batch classifier loss: 0.371564; batch adversarial loss: 0.647152\n",
      "epoch 41; iter: 0; batch classifier loss: 0.414006; batch adversarial loss: 0.633517\n",
      "epoch 41; iter: 200; batch classifier loss: 0.441817; batch adversarial loss: 0.534514\n",
      "epoch 42; iter: 0; batch classifier loss: 0.366043; batch adversarial loss: 0.616528\n",
      "epoch 42; iter: 200; batch classifier loss: 0.295737; batch adversarial loss: 0.596510\n",
      "epoch 43; iter: 0; batch classifier loss: 0.224757; batch adversarial loss: 0.670832\n",
      "epoch 43; iter: 200; batch classifier loss: 0.377527; batch adversarial loss: 0.577429\n",
      "epoch 44; iter: 0; batch classifier loss: 0.684909; batch adversarial loss: 0.586104\n",
      "epoch 44; iter: 200; batch classifier loss: 0.454930; batch adversarial loss: 0.601694\n",
      "epoch 45; iter: 0; batch classifier loss: 0.661890; batch adversarial loss: 0.599835\n",
      "epoch 45; iter: 200; batch classifier loss: 0.441343; batch adversarial loss: 0.665661\n",
      "epoch 46; iter: 0; batch classifier loss: 0.353501; batch adversarial loss: 0.620902\n",
      "epoch 46; iter: 200; batch classifier loss: 0.410432; batch adversarial loss: 0.591704\n",
      "epoch 47; iter: 0; batch classifier loss: 0.390072; batch adversarial loss: 0.581205\n",
      "epoch 47; iter: 200; batch classifier loss: 0.449626; batch adversarial loss: 0.584032\n",
      "epoch 48; iter: 0; batch classifier loss: 0.328125; batch adversarial loss: 0.618892\n",
      "epoch 48; iter: 200; batch classifier loss: 0.363862; batch adversarial loss: 0.640354\n",
      "epoch 49; iter: 0; batch classifier loss: 0.312614; batch adversarial loss: 0.630645\n",
      "epoch 49; iter: 200; batch classifier loss: 0.290630; batch adversarial loss: 0.638431\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import AdultDataset\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "adult_dataset = AdultDataset()\n",
    "adult_train, adult_test = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_privileged_groups = [{'sex': 1}]  # Male\n",
    "adult_unprivileged_groups = [{'sex': 0}]  # Female\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"adult_debiasing\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_ad.fit(adult_train)\n",
    "adult_predict = adult_ad.predict(adult_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b31f4-ab1f-4d6f-8427-afbd0d70b340",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de989d9-7cce-47aa-bcbb-99852859112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c04c799-ab8f-48dd-b3cf-3a489d9385c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_metric_test = ClassificationMetric(\n",
    "    adult_test,\n",
    "    adult_predict,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    privileged_groups=adult_privileged_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9482f105-687b-462f-ac79-a8c1e5b84e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.833419326306479), 'Precision': np.float64(0.7505731315910132), 'Equal Opportunity Difference': np.float64(0.17897872422469224), 'Statistical Parity Difference': np.float64(-0.0572880754242837)}\n"
     ]
    }
   ],
   "source": [
    "print({\n",
    "        \"Accuracy\": adult_metric_test.accuracy(),\n",
    "        \"Precision\":  adult_metric_test.precision(),\n",
    "        \"Equal Opportunity Difference\": adult_metric_test.equal_opportunity_difference(),\n",
    "        \"Statistical Parity Difference\": adult_metric_test.statistical_parity_difference(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802bda7-a794-4112-a584-7d809adad292",
   "metadata": {},
   "source": [
    "As statistical parity differenece is negative, there still is bias againt unprivileged group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afd7d3-6ff8-49ad-bf82-aa0e7d1a9874",
   "metadata": {},
   "source": [
    "### sensitive attribute = race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee989d9-9b6d-463a-824b-40736521158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_privileged_groups = [{'race': 1}]  # white\n",
    "adult_unprivileged_groups = [{'race': 0}]  # non-white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac050ae4-c00a-4b85-aa01-dd1dcd2709c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 9.611891; batch adversarial loss: 0.782873\n",
      "epoch 0; iter: 200; batch classifier loss: 11.640459; batch adversarial loss: 1.001259\n",
      "epoch 1; iter: 0; batch classifier loss: 25.443785; batch adversarial loss: 0.918664\n",
      "epoch 1; iter: 200; batch classifier loss: 26.085617; batch adversarial loss: 0.680850\n",
      "epoch 2; iter: 0; batch classifier loss: 1.541882; batch adversarial loss: 0.580884\n",
      "epoch 2; iter: 200; batch classifier loss: 3.901371; batch adversarial loss: 0.574877\n",
      "epoch 3; iter: 0; batch classifier loss: 0.714578; batch adversarial loss: 0.575285\n",
      "epoch 3; iter: 200; batch classifier loss: 2.649572; batch adversarial loss: 0.482377\n",
      "epoch 4; iter: 0; batch classifier loss: 1.768612; batch adversarial loss: 0.495498\n",
      "epoch 4; iter: 200; batch classifier loss: 1.800842; batch adversarial loss: 0.498334\n",
      "epoch 5; iter: 0; batch classifier loss: 1.512531; batch adversarial loss: 0.481856\n",
      "epoch 5; iter: 200; batch classifier loss: 1.164107; batch adversarial loss: 0.400188\n",
      "epoch 6; iter: 0; batch classifier loss: 1.065957; batch adversarial loss: 0.446718\n",
      "epoch 6; iter: 200; batch classifier loss: 0.762832; batch adversarial loss: 0.368520\n",
      "epoch 7; iter: 0; batch classifier loss: 0.827791; batch adversarial loss: 0.383346\n",
      "epoch 7; iter: 200; batch classifier loss: 0.797612; batch adversarial loss: 0.456455\n",
      "epoch 8; iter: 0; batch classifier loss: 0.694061; batch adversarial loss: 0.481681\n",
      "epoch 8; iter: 200; batch classifier loss: 0.576305; batch adversarial loss: 0.387699\n",
      "epoch 9; iter: 0; batch classifier loss: 0.538494; batch adversarial loss: 0.408163\n",
      "epoch 9; iter: 200; batch classifier loss: 0.365690; batch adversarial loss: 0.349350\n",
      "epoch 10; iter: 0; batch classifier loss: 0.431913; batch adversarial loss: 0.481123\n",
      "epoch 10; iter: 200; batch classifier loss: 0.394875; batch adversarial loss: 0.377069\n",
      "epoch 11; iter: 0; batch classifier loss: 0.657373; batch adversarial loss: 0.346396\n",
      "epoch 11; iter: 200; batch classifier loss: 0.523842; batch adversarial loss: 0.379681\n",
      "epoch 12; iter: 0; batch classifier loss: 0.663406; batch adversarial loss: 0.375797\n",
      "epoch 12; iter: 200; batch classifier loss: 0.400499; batch adversarial loss: 0.442832\n",
      "epoch 13; iter: 0; batch classifier loss: 0.389816; batch adversarial loss: 0.433349\n",
      "epoch 13; iter: 200; batch classifier loss: 0.404585; batch adversarial loss: 0.403942\n",
      "epoch 14; iter: 0; batch classifier loss: 0.532160; batch adversarial loss: 0.363768\n",
      "epoch 14; iter: 200; batch classifier loss: 0.510578; batch adversarial loss: 0.449485\n",
      "epoch 15; iter: 0; batch classifier loss: 0.470604; batch adversarial loss: 0.465696\n",
      "epoch 15; iter: 200; batch classifier loss: 0.417734; batch adversarial loss: 0.403055\n",
      "epoch 16; iter: 0; batch classifier loss: 0.378125; batch adversarial loss: 0.324806\n",
      "epoch 16; iter: 200; batch classifier loss: 0.429884; batch adversarial loss: 0.464856\n",
      "epoch 17; iter: 0; batch classifier loss: 0.356630; batch adversarial loss: 0.408005\n",
      "epoch 17; iter: 200; batch classifier loss: 0.364209; batch adversarial loss: 0.369879\n",
      "epoch 18; iter: 0; batch classifier loss: 0.389544; batch adversarial loss: 0.559165\n",
      "epoch 18; iter: 200; batch classifier loss: 0.415938; batch adversarial loss: 0.363386\n",
      "epoch 19; iter: 0; batch classifier loss: 0.357309; batch adversarial loss: 0.438847\n",
      "epoch 19; iter: 200; batch classifier loss: 0.385706; batch adversarial loss: 0.345628\n",
      "epoch 20; iter: 0; batch classifier loss: 0.296459; batch adversarial loss: 0.390001\n",
      "epoch 20; iter: 200; batch classifier loss: 0.363548; batch adversarial loss: 0.481335\n",
      "epoch 21; iter: 0; batch classifier loss: 0.397713; batch adversarial loss: 0.327919\n",
      "epoch 21; iter: 200; batch classifier loss: 0.355647; batch adversarial loss: 0.405937\n",
      "epoch 22; iter: 0; batch classifier loss: 0.450645; batch adversarial loss: 0.418538\n",
      "epoch 22; iter: 200; batch classifier loss: 0.406338; batch adversarial loss: 0.387678\n",
      "epoch 23; iter: 0; batch classifier loss: 0.401593; batch adversarial loss: 0.380774\n",
      "epoch 23; iter: 200; batch classifier loss: 0.311137; batch adversarial loss: 0.454564\n",
      "epoch 24; iter: 0; batch classifier loss: 0.543069; batch adversarial loss: 0.447894\n",
      "epoch 24; iter: 200; batch classifier loss: 0.292188; batch adversarial loss: 0.302902\n",
      "epoch 25; iter: 0; batch classifier loss: 0.387716; batch adversarial loss: 0.328888\n",
      "epoch 25; iter: 200; batch classifier loss: 0.334376; batch adversarial loss: 0.389953\n",
      "epoch 26; iter: 0; batch classifier loss: 0.276887; batch adversarial loss: 0.481570\n",
      "epoch 26; iter: 200; batch classifier loss: 0.290610; batch adversarial loss: 0.395362\n",
      "epoch 27; iter: 0; batch classifier loss: 0.452777; batch adversarial loss: 0.389700\n",
      "epoch 27; iter: 200; batch classifier loss: 0.426226; batch adversarial loss: 0.347023\n",
      "epoch 28; iter: 0; batch classifier loss: 0.295686; batch adversarial loss: 0.389283\n",
      "epoch 28; iter: 200; batch classifier loss: 0.334612; batch adversarial loss: 0.507997\n",
      "epoch 29; iter: 0; batch classifier loss: 0.371519; batch adversarial loss: 0.391060\n",
      "epoch 29; iter: 200; batch classifier loss: 0.360188; batch adversarial loss: 0.326978\n",
      "epoch 30; iter: 0; batch classifier loss: 0.315857; batch adversarial loss: 0.417967\n",
      "epoch 30; iter: 200; batch classifier loss: 0.290131; batch adversarial loss: 0.375800\n",
      "epoch 31; iter: 0; batch classifier loss: 0.295875; batch adversarial loss: 0.442699\n",
      "epoch 31; iter: 200; batch classifier loss: 0.369840; batch adversarial loss: 0.388198\n",
      "epoch 32; iter: 0; batch classifier loss: 0.361633; batch adversarial loss: 0.435463\n",
      "epoch 32; iter: 200; batch classifier loss: 0.398237; batch adversarial loss: 0.396831\n",
      "epoch 33; iter: 0; batch classifier loss: 0.406055; batch adversarial loss: 0.309661\n",
      "epoch 33; iter: 200; batch classifier loss: 0.279206; batch adversarial loss: 0.321761\n",
      "epoch 34; iter: 0; batch classifier loss: 0.331788; batch adversarial loss: 0.420818\n",
      "epoch 34; iter: 200; batch classifier loss: 0.364910; batch adversarial loss: 0.408070\n",
      "epoch 35; iter: 0; batch classifier loss: 0.381757; batch adversarial loss: 0.368995\n",
      "epoch 35; iter: 200; batch classifier loss: 0.335258; batch adversarial loss: 0.402065\n",
      "epoch 36; iter: 0; batch classifier loss: 0.318354; batch adversarial loss: 0.492998\n",
      "epoch 36; iter: 200; batch classifier loss: 0.413893; batch adversarial loss: 0.459054\n",
      "epoch 37; iter: 0; batch classifier loss: 0.362322; batch adversarial loss: 0.453690\n",
      "epoch 37; iter: 200; batch classifier loss: 0.437660; batch adversarial loss: 0.374769\n",
      "epoch 38; iter: 0; batch classifier loss: 0.334874; batch adversarial loss: 0.426294\n",
      "epoch 38; iter: 200; batch classifier loss: 0.289977; batch adversarial loss: 0.414853\n",
      "epoch 39; iter: 0; batch classifier loss: 0.348747; batch adversarial loss: 0.404896\n",
      "epoch 39; iter: 200; batch classifier loss: 0.306165; batch adversarial loss: 0.568043\n",
      "epoch 40; iter: 0; batch classifier loss: 0.292048; batch adversarial loss: 0.324407\n",
      "epoch 40; iter: 200; batch classifier loss: 0.392504; batch adversarial loss: 0.413571\n",
      "epoch 41; iter: 0; batch classifier loss: 0.403996; batch adversarial loss: 0.495722\n",
      "epoch 41; iter: 200; batch classifier loss: 0.384640; batch adversarial loss: 0.384941\n",
      "epoch 42; iter: 0; batch classifier loss: 0.400524; batch adversarial loss: 0.501964\n",
      "epoch 42; iter: 200; batch classifier loss: 0.345819; batch adversarial loss: 0.388777\n",
      "epoch 43; iter: 0; batch classifier loss: 0.389631; batch adversarial loss: 0.390537\n",
      "epoch 43; iter: 200; batch classifier loss: 0.478420; batch adversarial loss: 0.311937\n",
      "epoch 44; iter: 0; batch classifier loss: 0.526967; batch adversarial loss: 0.305604\n",
      "epoch 44; iter: 200; batch classifier loss: 0.325827; batch adversarial loss: 0.360945\n",
      "epoch 45; iter: 0; batch classifier loss: 0.322571; batch adversarial loss: 0.422701\n",
      "epoch 45; iter: 200; batch classifier loss: 0.443847; batch adversarial loss: 0.388873\n",
      "epoch 46; iter: 0; batch classifier loss: 0.284107; batch adversarial loss: 0.486327\n",
      "epoch 46; iter: 200; batch classifier loss: 0.506137; batch adversarial loss: 0.385864\n",
      "epoch 47; iter: 0; batch classifier loss: 0.413111; batch adversarial loss: 0.433013\n",
      "epoch 47; iter: 200; batch classifier loss: 0.367021; batch adversarial loss: 0.411750\n",
      "epoch 48; iter: 0; batch classifier loss: 0.438410; batch adversarial loss: 0.346707\n",
      "epoch 48; iter: 200; batch classifier loss: 0.340153; batch adversarial loss: 0.405444\n",
      "epoch 49; iter: 0; batch classifier loss: 0.392513; batch adversarial loss: 0.413926\n",
      "epoch 49; iter: 200; batch classifier loss: 0.435768; batch adversarial loss: 0.441039\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad_race = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"adult_debiasing_race\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_train_race, adult_test_race = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_ad_race.fit(adult_train_race)\n",
    "adult_predict_race = adult_ad_race.predict(adult_test_race)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd114b3-f52f-49ab-a471-8ca1040c452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.7997346502542935), 'Precision': np.float64(0.6807570977917982), 'Equal Opportunity Difference': np.float64(0.06123116278622465), 'Statistical Parity Difference': np.float64(-0.011656239262248)}\n"
     ]
    }
   ],
   "source": [
    "adult_race_metric_test = ClassificationMetric(\n",
    "    adult_test_race,\n",
    "    adult_predict_race,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    privileged_groups=adult_privileged_groups,\n",
    ")\n",
    "print({\n",
    "        \"Accuracy\": adult_race_metric_test.accuracy(),\n",
    "        \"Precision\":  adult_race_metric_test.precision(),\n",
    "        \"Equal Opportunity Difference\": adult_race_metric_test.equal_opportunity_difference(),\n",
    "        \"Statistical Parity Difference\": adult_race_metric_test.statistical_parity_difference(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41313c13-40c0-49cd-a7ae-da3cc47b1559",
   "metadata": {},
   "source": [
    "Adversary debiasing achieves better fairness performance when debiasing protected variable **race**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674166f1-9ba7-4ced-8b55-6151b42e9c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
