{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4wbUjK5aNFW"
   },
   "source": [
    "### Dependencies install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxZ5mJ3vEWJa"
   },
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4gkG3bOD7jU",
    "outputId": "ee812dbb-ee1a-4f5b-b5bb-d4da66d4c03e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoBuhiWPD_VJ",
    "outputId": "aa6da94d-32f3-4c40-c592-7d9f10838ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "# metadata\n",
    "print(adult.metadata)\n",
    "\n",
    "# variable information\n",
    "print(adult.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWRyY-AfEhlC"
   },
   "source": [
    "### Import IBM fairness 360 toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wERDXJU4ENRn",
    "outputId": "58def36f-4f0a-4e4c-fa09-64cbc379525e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aif360 --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "koe36LcMITdu"
   },
   "outputs": [],
   "source": [
    "import aif360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1vnOLIvF0pZ"
   },
   "source": [
    "### Adversarial debaising impl\n",
    "Ref: https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/inprocessing/adversarial_debiasing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfEhrv2aEp6a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow.compat.v1 as tf\n",
    "except ImportError as error:\n",
    "    from logging import warning\n",
    "    warning(\"{}: AdversarialDebiasing will be unavailable. To install, run:\\n\"\n",
    "            \"pip install 'aif360[AdversarialDebiasing]'\".format(error))\n",
    "\n",
    "from aif360.algorithms import Transformer\n",
    "\n",
    "\n",
    "class AdversarialDebiasing(Transformer):\n",
    "    \"\"\"Adversarial debiasing is an in-processing technique that learns a\n",
    "    classifier to maximize prediction accuracy and simultaneously reduce an\n",
    "    adversary's ability to determine the protected attribute from the\n",
    "    predictions [5]_. This approach leads to a fair classifier as the\n",
    "    predictions cannot carry any group discrimination information that the\n",
    "    adversary can exploit.\n",
    "\n",
    "    References:\n",
    "        .. [5] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating Unwanted\n",
    "           Biases with Adversarial Learning,\" AAAI/ACM Conference on Artificial\n",
    "           Intelligence, Ethics, and Society, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unprivileged_groups,\n",
    "                 privileged_groups,\n",
    "                 scope_name,\n",
    "                 sess,\n",
    "                 seed=None,\n",
    "                 adversary_loss_weight=0.1,\n",
    "                 num_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 classifier_num_hidden_units=200,\n",
    "                 debias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (tuple): Representation for unprivileged groups\n",
    "            privileged_groups (tuple): Representation for privileged groups\n",
    "            scope_name (str): scope name for the tenforflow variables\n",
    "            sess (tf.Session): tensorflow session\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "            adversary_loss_weight (float, optional): Hyperparameter that chooses\n",
    "                the strength of the adversarial loss.\n",
    "            num_epochs (int, optional): Number of training epochs.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            classifier_num_hidden_units (int, optional): Number of hidden units\n",
    "                in the classifier model.\n",
    "            debias (bool, optional): Learn a classifier with or without\n",
    "                debiasing.\n",
    "        \"\"\"\n",
    "        super(AdversarialDebiasing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "        if len(self.unprivileged_groups) > 1 or len(self.privileged_groups) > 1:\n",
    "            raise ValueError(\"Only one unprivileged_group or privileged_group supported.\")\n",
    "        self.protected_attribute_name = list(self.unprivileged_groups[0].keys())[0]\n",
    "\n",
    "        self.sess = sess\n",
    "        self.adversary_loss_weight = adversary_loss_weight\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
    "        self.debias = debias\n",
    "\n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def _classifier_model(self, features, features_dim, keep_prob):\n",
    "        \"\"\"Compute the classifier predictions for the outcome variable.\n",
    "\n",
    "        Input\n",
    "        features: a tensor representing the input features for the classifier\n",
    "        features_dim: dimension of the feature dataset\n",
    "\n",
    "        Return\n",
    "        pred_label: predicted label\n",
    "        pred_logits: raw logits output from the output layer that holds\n",
    "        the unactivated score of prediction confidence\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"classifier_model\"):\n",
    "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
    "                                  initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
    "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
    "\n",
    "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_logit = tf.matmul(h1, W2) + b2\n",
    "            pred_label = tf.sigmoid(pred_logit) # predictive binary classification with sigmoid activation\n",
    "\n",
    "        return pred_label, pred_logit\n",
    "\n",
    "    def _adversary_model(self, pred_logits, true_labels):\n",
    "        \"\"\"Compute the adversary predictions for the protected attribute.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [3, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_protected_attribute_logit = tf.matmul(tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1), W2) + b2\n",
    "            pred_protected_attribute_label = tf.sigmoid(pred_protected_attribute_logit)\n",
    "\n",
    "        return pred_protected_attribute_label, pred_protected_attribute_logit\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Compute the model parameters of the fair classifier using gradient\n",
    "        descent.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "\n",
    "        Returns:\n",
    "            AdversarialDebiasing: Returns self.\n",
    "        \"\"\"\n",
    "        if tf.executing_eagerly():\n",
    "            raise RuntimeError(\"AdversarialDebiasing does not work in eager \"\n",
    "                    \"execution mode. To fix, add `tf.disable_eager_execution()`\"\n",
    "                    \" to the top of the calling script.\")\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        ii32 = np.iinfo(np.int32)\n",
    "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(ii32.min, ii32.max, size=4)\n",
    "\n",
    "        # Map the dataset labels to 0 and 1.\n",
    "        temp_labels = dataset.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(),0] = 1.0\n",
    "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(),0] = 0.0\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            # cross entropy loss between true and predicted labels\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(pred_logits, self.true_labels_ph)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                       1000, 0.96, staircase=True)\n",
    "            # Tensorflow optimizer for classifier\n",
    "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                # Tensorflow optimizer for adversary\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            classifier_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'classifier_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'adversary_model' in var.name]\n",
    "                # Update classifier parameters\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n",
    "                                                                                      var_list=classifier_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "            classifier_grads = []\n",
    "            for (grad,var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                classifier_grads.append((grad, var))\n",
    "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                with tf.control_dependencies([classifier_minimizer]):\n",
    "                    adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars)#, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = dataset.features[batch_ids]\n",
    "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                                 dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                       self.true_labels_ph: batch_labels,\n",
    "                                       self.protected_attributes_ph: batch_protected_attributes,\n",
    "                                       self.keep_prob: 0.8}\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([classifier_minimizer,\n",
    "                                       adversary_minimizer,\n",
    "                                       pred_labels_loss,\n",
    "                                       pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run(\n",
    "                            [classifier_minimizer,\n",
    "                             pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair\n",
    "        classifier learned.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "        Returns:\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(dataset.features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = dataset.features[batch_ids]\n",
    "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                         dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy(deepcopy = True)\n",
    "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
    "        dataset_new.labels = (np.array(pred_labels)>0.5).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Map the dataset labels to back to their original values.\n",
    "        temp_labels = dataset_new.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
    "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
    "\n",
    "        dataset_new.labels = temp_labels.copy()\n",
    "\n",
    "        return dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQqWB9XZd0ou"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_MEvoj-9yu-"
   },
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'aif360[AdversarialDebiasing,Reductions,inFairness]' --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jN_MEP6T9zDD"
   },
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "def evaluate_binary_label_metric(\n",
    "    data,\n",
    "    label_col,\n",
    "    sensitive_col,\n",
    "    privileged_val,\n",
    "    unprivileged_val,\n",
    "):\n",
    "    \"\"\"Evaluate fairness metrics using BinaryLabelDatasetMetric on a single dataset\"\"\"\n",
    "    dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=data,\n",
    "        label_names=[label_col],\n",
    "        protected_attribute_names=[sensitive_col]\n",
    "    )\n",
    "    \n",
    "    metric = BinaryLabelDatasetMetric(\n",
    "        dataset,\n",
    "        privileged_groups=[{sensitive_col: privileged_val}],\n",
    "        unprivileged_groups=[{sensitive_col: unprivileged_val}]\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        \"Statistical Parity Difference\": metric.statistical_parity_difference(),\n",
    "        \"Disparate Impact\": metric.disparate_impact(),\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def evaluate_classification_metric(\n",
    "    data,\n",
    "    label_col,\n",
    "    pred_col,\n",
    "    sensitive_col,\n",
    "    privileged_val,\n",
    "    unprivileged_val,\n",
    "):\n",
    "    \"\"\"Evaluates fairness metrics using ClassificationMetric for original and predicted datasets\"\"\"\n",
    "    # Original dataset\n",
    "    dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=data,\n",
    "        label_names=[label_col],\n",
    "        protected_attribute_names=[sensitive_col]\n",
    "    )\n",
    "    \n",
    "    # Predicted dataset\n",
    "    predicted_dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=data,\n",
    "        label_names=[pred_col],\n",
    "        protected_attribute_names=[sensitive_col]\n",
    "    )\n",
    "    \n",
    "    metric = ClassificationMetric(\n",
    "        dataset,\n",
    "        predicted_dataset,\n",
    "        privileged_groups=[{sensitive_col: privileged_val}],\n",
    "        unprivileged_groups=[{sensitive_col: unprivileged_val}]\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        \"Equal Opportunity Difference\": metric.equal_opportunity_difference(),\n",
    "        \"Average Odds Difference\": metric.average_odds_difference(),\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
