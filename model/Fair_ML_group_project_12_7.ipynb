{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc812d2-f38f-4ea8-98ae-de098ec27960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow.compat.v1 as tf\n",
    "except ImportError as error:\n",
    "    from logging import warning\n",
    "    warning(\"{}: AdversarialDebiasing will be unavailable. To install, run:\\n\"\n",
    "            \"pip install 'aif360[AdversarialDebiasing]'\".format(error))\n",
    "\n",
    "from aif360.algorithms import Transformer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11864e41-e105-4f29-bff2-9b4c45e3b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class AdversarialDebiasing(Transformer):\n",
    "    \"\"\"Adversarial debiasing is an in-processing technique that learns a\n",
    "    classifier to maximize prediction accuracy and simultaneously reduce an\n",
    "    adversary's ability to determine the protected attribute from the\n",
    "    predictions [5]_. This approach leads to a fair classifier as the\n",
    "    predictions cannot carry any group discrimination information that the\n",
    "    adversary can exploit.\n",
    "\n",
    "    References:\n",
    "        .. [5] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating Unwanted\n",
    "           Biases with Adversarial Learning,\" AAAI/ACM Conference on Artificial\n",
    "           Intelligence, Ethics, and Society, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unprivileged_groups,\n",
    "                 privileged_groups,\n",
    "                 scope_name,\n",
    "                 sess,\n",
    "                 seed=None,\n",
    "                 adversary_loss_weight=0.1,\n",
    "                 num_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 classifier_num_hidden_units=200,\n",
    "                 debias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (tuple): Representation for unprivileged groups\n",
    "            privileged_groups (tuple): Representation for privileged groups\n",
    "            scope_name (str): scope name for the tenforflow variables\n",
    "            sess (tf.Session): tensorflow session\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "            adversary_loss_weight (float, optional): Hyperparameter that chooses\n",
    "                the strength of the adversarial loss.\n",
    "            num_epochs (int, optional): Number of training epochs.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            classifier_num_hidden_units (int, optional): Number of hidden units\n",
    "                in the classifier model.\n",
    "            debias (bool, optional): Learn a classifier with or without\n",
    "                debiasing.\n",
    "        \"\"\"\n",
    "        super(AdversarialDebiasing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "        if len(self.unprivileged_groups) > 1 or len(self.privileged_groups) > 1:\n",
    "            raise ValueError(\"Only one unprivileged_group or privileged_group supported.\")\n",
    "        self.protected_attribute_name = list(self.unprivileged_groups[0].keys())[0]\n",
    "\n",
    "        self.sess = sess\n",
    "        self.adversary_loss_weight = adversary_loss_weight\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
    "        self.debias = debias\n",
    "\n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def _classifier_model(self, features, features_dim, keep_prob):\n",
    "        \"\"\"Compute the classifier predictions for the outcome variable.\n",
    "\n",
    "        Input\n",
    "        features: a tensor representing the input features for the classifier\n",
    "        features_dim: dimension of the feature dataset\n",
    "\n",
    "        Return\n",
    "        pred_label: predicted label\n",
    "        pred_logits: raw logits output from the output layer that holds\n",
    "        the unactivated score of prediction confidence\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"classifier_model\"):\n",
    "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
    "                                  initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
    "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
    "\n",
    "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_logit = tf.matmul(h1, W2) + b2\n",
    "            pred_label = tf.sigmoid(pred_logit) # predictive binary classification with sigmoid activation\n",
    "\n",
    "        return pred_label, pred_logit\n",
    "\n",
    "    def _adversary_model(self, pred_logits, true_labels):\n",
    "        \"\"\"Compute the adversary predictions for the protected attribute.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [3, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_protected_attribute_logit = tf.matmul(tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1), W2) + b2\n",
    "            pred_protected_attribute_label = tf.sigmoid(pred_protected_attribute_logit)\n",
    "\n",
    "        return pred_protected_attribute_label, pred_protected_attribute_logit\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Compute the model parameters of the fair classifier using gradient\n",
    "        descent.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "\n",
    "        Returns:\n",
    "            AdversarialDebiasing: Returns self.\n",
    "        \"\"\"\n",
    "        if tf.executing_eagerly():\n",
    "            raise RuntimeError(\"AdversarialDebiasing does not work in eager \"\n",
    "                    \"execution mode. To fix, add `tf.disable_eager_execution()`\"\n",
    "                    \" to the top of the calling script.\")\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        ii32 = np.iinfo(np.int32)\n",
    "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(ii32.min, ii32.max, size=4)\n",
    "\n",
    "        # Map the dataset labels to 0 and 1.\n",
    "        temp_labels = dataset.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(),0] = 1.0\n",
    "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(),0] = 0.0\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            # cross entropy loss between true and predicted labels\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(pred_logits, self.true_labels_ph)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                       1000, 0.96, staircase=True)\n",
    "            # Tensorflow optimizer for classifier\n",
    "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                # Tensorflow optimizer for adversary\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            classifier_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'classifier_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'adversary_model' in var.name]\n",
    "                # Update classifier parameters\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n",
    "                                                                                      var_list=classifier_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "            classifier_grads = []\n",
    "            for (grad,var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                classifier_grads.append((grad, var))\n",
    "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                with tf.control_dependencies([classifier_minimizer]):\n",
    "                    adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars)#, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = dataset.features[batch_ids]\n",
    "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                                 dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                       self.true_labels_ph: batch_labels,\n",
    "                                       self.protected_attributes_ph: batch_protected_attributes,\n",
    "                                       self.keep_prob: 0.8}\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([classifier_minimizer,\n",
    "                                       adversary_minimizer,\n",
    "                                       pred_labels_loss,\n",
    "                                       pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run(\n",
    "                            [classifier_minimizer,\n",
    "                             pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair\n",
    "        classifier learned.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "        Returns:\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(dataset.features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = dataset.features[batch_ids]\n",
    "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                         dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy(deepcopy = True)\n",
    "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
    "        dataset_new.labels = (np.array(pred_labels)>0.5).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Map the dataset labels to back to their original values.\n",
    "        temp_labels = dataset_new.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
    "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
    "\n",
    "        dataset_new.labels = temp_labels.copy()\n",
    "\n",
    "        return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daacc6-d746-4b12-b7aa-c695f4720a7a",
   "metadata": {},
   "source": [
    "# Test local AD impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89447823-c5b9-466b-8507-47dbaf264f7c",
   "metadata": {},
   "source": [
    "## Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77feeaee-f888-4a6d-acc5-2ec9c6075194",
   "metadata": {},
   "source": [
    "### sensitive attribute = sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71038c9-d632-43c2-9970-c22bfe750226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Sandy/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Sandy/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 48.314850; batch adversarial loss: 1.134260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733092916.767180  639727 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 200; batch classifier loss: 10.646967; batch adversarial loss: 1.049885\n",
      "epoch 1; iter: 0; batch classifier loss: 8.462988; batch adversarial loss: 0.757050\n",
      "epoch 1; iter: 200; batch classifier loss: 5.221793; batch adversarial loss: 0.823377\n",
      "epoch 2; iter: 0; batch classifier loss: 2.613160; batch adversarial loss: 0.752772\n",
      "epoch 2; iter: 200; batch classifier loss: 4.986633; batch adversarial loss: 0.707846\n",
      "epoch 3; iter: 0; batch classifier loss: 5.351168; batch adversarial loss: 0.688093\n",
      "epoch 3; iter: 200; batch classifier loss: 2.172413; batch adversarial loss: 0.664870\n",
      "epoch 4; iter: 0; batch classifier loss: 1.712467; batch adversarial loss: 0.616332\n",
      "epoch 4; iter: 200; batch classifier loss: 1.110458; batch adversarial loss: 0.625024\n",
      "epoch 5; iter: 0; batch classifier loss: 0.852619; batch adversarial loss: 0.578249\n",
      "epoch 5; iter: 200; batch classifier loss: 0.576080; batch adversarial loss: 0.646356\n",
      "epoch 6; iter: 0; batch classifier loss: 1.557930; batch adversarial loss: 0.640221\n",
      "epoch 6; iter: 200; batch classifier loss: 0.746857; batch adversarial loss: 0.640421\n",
      "epoch 7; iter: 0; batch classifier loss: 1.198074; batch adversarial loss: 0.678730\n",
      "epoch 7; iter: 200; batch classifier loss: 0.571567; batch adversarial loss: 0.654624\n",
      "epoch 8; iter: 0; batch classifier loss: 0.601516; batch adversarial loss: 0.609715\n",
      "epoch 8; iter: 200; batch classifier loss: 0.405702; batch adversarial loss: 0.620137\n",
      "epoch 9; iter: 0; batch classifier loss: 0.595322; batch adversarial loss: 0.609998\n",
      "epoch 9; iter: 200; batch classifier loss: 0.580970; batch adversarial loss: 0.637152\n",
      "epoch 10; iter: 0; batch classifier loss: 0.487276; batch adversarial loss: 0.648509\n",
      "epoch 10; iter: 200; batch classifier loss: 0.338899; batch adversarial loss: 0.629343\n",
      "epoch 11; iter: 0; batch classifier loss: 0.350056; batch adversarial loss: 0.563106\n",
      "epoch 11; iter: 200; batch classifier loss: 0.374812; batch adversarial loss: 0.609130\n",
      "epoch 12; iter: 0; batch classifier loss: 0.401039; batch adversarial loss: 0.634886\n",
      "epoch 12; iter: 200; batch classifier loss: 0.395386; batch adversarial loss: 0.600267\n",
      "epoch 13; iter: 0; batch classifier loss: 0.372854; batch adversarial loss: 0.598762\n",
      "epoch 13; iter: 200; batch classifier loss: 0.543044; batch adversarial loss: 0.612449\n",
      "epoch 14; iter: 0; batch classifier loss: 0.501084; batch adversarial loss: 0.625467\n",
      "epoch 14; iter: 200; batch classifier loss: 0.400182; batch adversarial loss: 0.613020\n",
      "epoch 15; iter: 0; batch classifier loss: 0.622033; batch adversarial loss: 0.639541\n",
      "epoch 15; iter: 200; batch classifier loss: 0.424642; batch adversarial loss: 0.672964\n",
      "epoch 16; iter: 0; batch classifier loss: 0.365013; batch adversarial loss: 0.569588\n",
      "epoch 16; iter: 200; batch classifier loss: 0.418848; batch adversarial loss: 0.609323\n",
      "epoch 17; iter: 0; batch classifier loss: 0.362498; batch adversarial loss: 0.627431\n",
      "epoch 17; iter: 200; batch classifier loss: 0.459670; batch adversarial loss: 0.623106\n",
      "epoch 18; iter: 0; batch classifier loss: 0.426325; batch adversarial loss: 0.621433\n",
      "epoch 18; iter: 200; batch classifier loss: 0.471884; batch adversarial loss: 0.629325\n",
      "epoch 19; iter: 0; batch classifier loss: 0.429050; batch adversarial loss: 0.565156\n",
      "epoch 19; iter: 200; batch classifier loss: 0.534661; batch adversarial loss: 0.580990\n",
      "epoch 20; iter: 0; batch classifier loss: 0.380726; batch adversarial loss: 0.627517\n",
      "epoch 20; iter: 200; batch classifier loss: 0.361115; batch adversarial loss: 0.634588\n",
      "epoch 21; iter: 0; batch classifier loss: 0.384497; batch adversarial loss: 0.559202\n",
      "epoch 21; iter: 200; batch classifier loss: 0.374945; batch adversarial loss: 0.612513\n",
      "epoch 22; iter: 0; batch classifier loss: 0.423483; batch adversarial loss: 0.617644\n",
      "epoch 22; iter: 200; batch classifier loss: 0.292608; batch adversarial loss: 0.660694\n",
      "epoch 23; iter: 0; batch classifier loss: 0.332288; batch adversarial loss: 0.636996\n",
      "epoch 23; iter: 200; batch classifier loss: 0.480683; batch adversarial loss: 0.612080\n",
      "epoch 24; iter: 0; batch classifier loss: 0.333658; batch adversarial loss: 0.677328\n",
      "epoch 24; iter: 200; batch classifier loss: 0.340440; batch adversarial loss: 0.607391\n",
      "epoch 25; iter: 0; batch classifier loss: 0.385499; batch adversarial loss: 0.611394\n",
      "epoch 25; iter: 200; batch classifier loss: 0.398684; batch adversarial loss: 0.535844\n",
      "epoch 26; iter: 0; batch classifier loss: 0.397044; batch adversarial loss: 0.605600\n",
      "epoch 26; iter: 200; batch classifier loss: 0.344291; batch adversarial loss: 0.604988\n",
      "epoch 27; iter: 0; batch classifier loss: 0.337300; batch adversarial loss: 0.635675\n",
      "epoch 27; iter: 200; batch classifier loss: 0.297019; batch adversarial loss: 0.661540\n",
      "epoch 28; iter: 0; batch classifier loss: 0.324038; batch adversarial loss: 0.586085\n",
      "epoch 28; iter: 200; batch classifier loss: 0.263948; batch adversarial loss: 0.622848\n",
      "epoch 29; iter: 0; batch classifier loss: 0.313512; batch adversarial loss: 0.612815\n",
      "epoch 29; iter: 200; batch classifier loss: 0.366802; batch adversarial loss: 0.553731\n",
      "epoch 30; iter: 0; batch classifier loss: 0.394665; batch adversarial loss: 0.631600\n",
      "epoch 30; iter: 200; batch classifier loss: 0.365675; batch adversarial loss: 0.604194\n",
      "epoch 31; iter: 0; batch classifier loss: 0.420337; batch adversarial loss: 0.663199\n",
      "epoch 31; iter: 200; batch classifier loss: 0.323618; batch adversarial loss: 0.621889\n",
      "epoch 32; iter: 0; batch classifier loss: 0.294043; batch adversarial loss: 0.614670\n",
      "epoch 32; iter: 200; batch classifier loss: 0.364143; batch adversarial loss: 0.604638\n",
      "epoch 33; iter: 0; batch classifier loss: 0.347116; batch adversarial loss: 0.660761\n",
      "epoch 33; iter: 200; batch classifier loss: 0.386212; batch adversarial loss: 0.613889\n",
      "epoch 34; iter: 0; batch classifier loss: 0.405549; batch adversarial loss: 0.675028\n",
      "epoch 34; iter: 200; batch classifier loss: 0.312177; batch adversarial loss: 0.605645\n",
      "epoch 35; iter: 0; batch classifier loss: 0.329889; batch adversarial loss: 0.553440\n",
      "epoch 35; iter: 200; batch classifier loss: 0.296043; batch adversarial loss: 0.621281\n",
      "epoch 36; iter: 0; batch classifier loss: 0.331361; batch adversarial loss: 0.603651\n",
      "epoch 36; iter: 200; batch classifier loss: 0.279556; batch adversarial loss: 0.615458\n",
      "epoch 37; iter: 0; batch classifier loss: 0.378815; batch adversarial loss: 0.645769\n",
      "epoch 37; iter: 200; batch classifier loss: 0.466447; batch adversarial loss: 0.644004\n",
      "epoch 38; iter: 0; batch classifier loss: 0.304113; batch adversarial loss: 0.661224\n",
      "epoch 38; iter: 200; batch classifier loss: 0.287013; batch adversarial loss: 0.637390\n",
      "epoch 39; iter: 0; batch classifier loss: 0.425049; batch adversarial loss: 0.599046\n",
      "epoch 39; iter: 200; batch classifier loss: 0.395046; batch adversarial loss: 0.634162\n",
      "epoch 40; iter: 0; batch classifier loss: 0.345297; batch adversarial loss: 0.579250\n",
      "epoch 40; iter: 200; batch classifier loss: 0.371564; batch adversarial loss: 0.647152\n",
      "epoch 41; iter: 0; batch classifier loss: 0.414006; batch adversarial loss: 0.633517\n",
      "epoch 41; iter: 200; batch classifier loss: 0.441817; batch adversarial loss: 0.534514\n",
      "epoch 42; iter: 0; batch classifier loss: 0.366043; batch adversarial loss: 0.616528\n",
      "epoch 42; iter: 200; batch classifier loss: 0.295737; batch adversarial loss: 0.596510\n",
      "epoch 43; iter: 0; batch classifier loss: 0.224757; batch adversarial loss: 0.670832\n",
      "epoch 43; iter: 200; batch classifier loss: 0.377527; batch adversarial loss: 0.577429\n",
      "epoch 44; iter: 0; batch classifier loss: 0.684909; batch adversarial loss: 0.586104\n",
      "epoch 44; iter: 200; batch classifier loss: 0.454930; batch adversarial loss: 0.601694\n",
      "epoch 45; iter: 0; batch classifier loss: 0.661890; batch adversarial loss: 0.599835\n",
      "epoch 45; iter: 200; batch classifier loss: 0.441343; batch adversarial loss: 0.665661\n",
      "epoch 46; iter: 0; batch classifier loss: 0.353501; batch adversarial loss: 0.620902\n",
      "epoch 46; iter: 200; batch classifier loss: 0.410432; batch adversarial loss: 0.591704\n",
      "epoch 47; iter: 0; batch classifier loss: 0.390072; batch adversarial loss: 0.581205\n",
      "epoch 47; iter: 200; batch classifier loss: 0.449626; batch adversarial loss: 0.584032\n",
      "epoch 48; iter: 0; batch classifier loss: 0.328125; batch adversarial loss: 0.618892\n",
      "epoch 48; iter: 200; batch classifier loss: 0.363862; batch adversarial loss: 0.640354\n",
      "epoch 49; iter: 0; batch classifier loss: 0.312614; batch adversarial loss: 0.630645\n",
      "epoch 49; iter: 200; batch classifier loss: 0.290630; batch adversarial loss: 0.638431\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import AdultDataset\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "adult_dataset = AdultDataset()\n",
    "adult_train, adult_test = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_privileged_groups = [{'sex': 1}]  # Male\n",
    "adult_unprivileged_groups = [{'sex': 0}]  # Female\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"adult_debiasing\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_ad.fit(adult_train)\n",
    "adult_predict = adult_ad.predict(adult_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b31f4-ab1f-4d6f-8427-afbd0d70b340",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de989d9-7cce-47aa-bcbb-99852859112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c04c799-ab8f-48dd-b3cf-3a489d9385c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_metric_test = ClassificationMetric(\n",
    "    adult_test,\n",
    "    adult_predict,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    privileged_groups=adult_privileged_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9482f105-687b-462f-ac79-a8c1e5b84e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.833419326306479), 'Precision': np.float64(0.7505731315910132), 'Equal Opportunity Difference': np.float64(0.17897872422469224), 'Statistical Parity Difference': np.float64(-0.0572880754242837)}\n"
     ]
    }
   ],
   "source": [
    "print({\n",
    "        \"Accuracy\": adult_metric_test.accuracy(),\n",
    "        \"Precision\":  adult_metric_test.precision(),\n",
    "        \"Equal Opportunity Difference\": adult_metric_test.equal_opportunity_difference(),\n",
    "        \"Statistical Parity Difference\": adult_metric_test.statistical_parity_difference(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802bda7-a794-4112-a584-7d809adad292",
   "metadata": {},
   "source": [
    "As statistical parity differenece is negative, there still is bias againt unprivileged group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afd7d3-6ff8-49ad-bf82-aa0e7d1a9874",
   "metadata": {},
   "source": [
    "### sensitive attribute = race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee989d9-9b6d-463a-824b-40736521158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_privileged_groups = [{'race': 1}]  # white\n",
    "adult_unprivileged_groups = [{'race': 0}]  # non-white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac050ae4-c00a-4b85-aa01-dd1dcd2709c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 9.611891; batch adversarial loss: 0.782873\n",
      "epoch 0; iter: 200; batch classifier loss: 11.640459; batch adversarial loss: 1.001259\n",
      "epoch 1; iter: 0; batch classifier loss: 25.443785; batch adversarial loss: 0.918664\n",
      "epoch 1; iter: 200; batch classifier loss: 26.085617; batch adversarial loss: 0.680850\n",
      "epoch 2; iter: 0; batch classifier loss: 1.541882; batch adversarial loss: 0.580884\n",
      "epoch 2; iter: 200; batch classifier loss: 3.901371; batch adversarial loss: 0.574877\n",
      "epoch 3; iter: 0; batch classifier loss: 0.714578; batch adversarial loss: 0.575285\n",
      "epoch 3; iter: 200; batch classifier loss: 2.649572; batch adversarial loss: 0.482377\n",
      "epoch 4; iter: 0; batch classifier loss: 1.768612; batch adversarial loss: 0.495498\n",
      "epoch 4; iter: 200; batch classifier loss: 1.800842; batch adversarial loss: 0.498334\n",
      "epoch 5; iter: 0; batch classifier loss: 1.512531; batch adversarial loss: 0.481856\n",
      "epoch 5; iter: 200; batch classifier loss: 1.164107; batch adversarial loss: 0.400188\n",
      "epoch 6; iter: 0; batch classifier loss: 1.065957; batch adversarial loss: 0.446718\n",
      "epoch 6; iter: 200; batch classifier loss: 0.762832; batch adversarial loss: 0.368520\n",
      "epoch 7; iter: 0; batch classifier loss: 0.827791; batch adversarial loss: 0.383346\n",
      "epoch 7; iter: 200; batch classifier loss: 0.797612; batch adversarial loss: 0.456455\n",
      "epoch 8; iter: 0; batch classifier loss: 0.694061; batch adversarial loss: 0.481681\n",
      "epoch 8; iter: 200; batch classifier loss: 0.576305; batch adversarial loss: 0.387699\n",
      "epoch 9; iter: 0; batch classifier loss: 0.538494; batch adversarial loss: 0.408163\n",
      "epoch 9; iter: 200; batch classifier loss: 0.365690; batch adversarial loss: 0.349350\n",
      "epoch 10; iter: 0; batch classifier loss: 0.431913; batch adversarial loss: 0.481123\n",
      "epoch 10; iter: 200; batch classifier loss: 0.394875; batch adversarial loss: 0.377069\n",
      "epoch 11; iter: 0; batch classifier loss: 0.657373; batch adversarial loss: 0.346396\n",
      "epoch 11; iter: 200; batch classifier loss: 0.523842; batch adversarial loss: 0.379681\n",
      "epoch 12; iter: 0; batch classifier loss: 0.663406; batch adversarial loss: 0.375797\n",
      "epoch 12; iter: 200; batch classifier loss: 0.400499; batch adversarial loss: 0.442832\n",
      "epoch 13; iter: 0; batch classifier loss: 0.389816; batch adversarial loss: 0.433349\n",
      "epoch 13; iter: 200; batch classifier loss: 0.404585; batch adversarial loss: 0.403942\n",
      "epoch 14; iter: 0; batch classifier loss: 0.532160; batch adversarial loss: 0.363768\n",
      "epoch 14; iter: 200; batch classifier loss: 0.510578; batch adversarial loss: 0.449485\n",
      "epoch 15; iter: 0; batch classifier loss: 0.470604; batch adversarial loss: 0.465696\n",
      "epoch 15; iter: 200; batch classifier loss: 0.417734; batch adversarial loss: 0.403055\n",
      "epoch 16; iter: 0; batch classifier loss: 0.378125; batch adversarial loss: 0.324806\n",
      "epoch 16; iter: 200; batch classifier loss: 0.429884; batch adversarial loss: 0.464856\n",
      "epoch 17; iter: 0; batch classifier loss: 0.356630; batch adversarial loss: 0.408005\n",
      "epoch 17; iter: 200; batch classifier loss: 0.364209; batch adversarial loss: 0.369879\n",
      "epoch 18; iter: 0; batch classifier loss: 0.389544; batch adversarial loss: 0.559165\n",
      "epoch 18; iter: 200; batch classifier loss: 0.415938; batch adversarial loss: 0.363386\n",
      "epoch 19; iter: 0; batch classifier loss: 0.357309; batch adversarial loss: 0.438847\n",
      "epoch 19; iter: 200; batch classifier loss: 0.385706; batch adversarial loss: 0.345628\n",
      "epoch 20; iter: 0; batch classifier loss: 0.296459; batch adversarial loss: 0.390001\n",
      "epoch 20; iter: 200; batch classifier loss: 0.363548; batch adversarial loss: 0.481335\n",
      "epoch 21; iter: 0; batch classifier loss: 0.397713; batch adversarial loss: 0.327919\n",
      "epoch 21; iter: 200; batch classifier loss: 0.355647; batch adversarial loss: 0.405937\n",
      "epoch 22; iter: 0; batch classifier loss: 0.450645; batch adversarial loss: 0.418538\n",
      "epoch 22; iter: 200; batch classifier loss: 0.406338; batch adversarial loss: 0.387678\n",
      "epoch 23; iter: 0; batch classifier loss: 0.401593; batch adversarial loss: 0.380774\n",
      "epoch 23; iter: 200; batch classifier loss: 0.311137; batch adversarial loss: 0.454564\n",
      "epoch 24; iter: 0; batch classifier loss: 0.543069; batch adversarial loss: 0.447894\n",
      "epoch 24; iter: 200; batch classifier loss: 0.292188; batch adversarial loss: 0.302902\n",
      "epoch 25; iter: 0; batch classifier loss: 0.387716; batch adversarial loss: 0.328888\n",
      "epoch 25; iter: 200; batch classifier loss: 0.334376; batch adversarial loss: 0.389953\n",
      "epoch 26; iter: 0; batch classifier loss: 0.276887; batch adversarial loss: 0.481570\n",
      "epoch 26; iter: 200; batch classifier loss: 0.290610; batch adversarial loss: 0.395362\n",
      "epoch 27; iter: 0; batch classifier loss: 0.452777; batch adversarial loss: 0.389700\n",
      "epoch 27; iter: 200; batch classifier loss: 0.426226; batch adversarial loss: 0.347023\n",
      "epoch 28; iter: 0; batch classifier loss: 0.295686; batch adversarial loss: 0.389283\n",
      "epoch 28; iter: 200; batch classifier loss: 0.334612; batch adversarial loss: 0.507997\n",
      "epoch 29; iter: 0; batch classifier loss: 0.371519; batch adversarial loss: 0.391060\n",
      "epoch 29; iter: 200; batch classifier loss: 0.360188; batch adversarial loss: 0.326978\n",
      "epoch 30; iter: 0; batch classifier loss: 0.315857; batch adversarial loss: 0.417967\n",
      "epoch 30; iter: 200; batch classifier loss: 0.290131; batch adversarial loss: 0.375800\n",
      "epoch 31; iter: 0; batch classifier loss: 0.295875; batch adversarial loss: 0.442699\n",
      "epoch 31; iter: 200; batch classifier loss: 0.369840; batch adversarial loss: 0.388198\n",
      "epoch 32; iter: 0; batch classifier loss: 0.361633; batch adversarial loss: 0.435463\n",
      "epoch 32; iter: 200; batch classifier loss: 0.398237; batch adversarial loss: 0.396831\n",
      "epoch 33; iter: 0; batch classifier loss: 0.406055; batch adversarial loss: 0.309661\n",
      "epoch 33; iter: 200; batch classifier loss: 0.279206; batch adversarial loss: 0.321761\n",
      "epoch 34; iter: 0; batch classifier loss: 0.331788; batch adversarial loss: 0.420818\n",
      "epoch 34; iter: 200; batch classifier loss: 0.364910; batch adversarial loss: 0.408070\n",
      "epoch 35; iter: 0; batch classifier loss: 0.381757; batch adversarial loss: 0.368995\n",
      "epoch 35; iter: 200; batch classifier loss: 0.335258; batch adversarial loss: 0.402065\n",
      "epoch 36; iter: 0; batch classifier loss: 0.318354; batch adversarial loss: 0.492998\n",
      "epoch 36; iter: 200; batch classifier loss: 0.413893; batch adversarial loss: 0.459054\n",
      "epoch 37; iter: 0; batch classifier loss: 0.362322; batch adversarial loss: 0.453690\n",
      "epoch 37; iter: 200; batch classifier loss: 0.437660; batch adversarial loss: 0.374769\n",
      "epoch 38; iter: 0; batch classifier loss: 0.334874; batch adversarial loss: 0.426294\n",
      "epoch 38; iter: 200; batch classifier loss: 0.289977; batch adversarial loss: 0.414853\n",
      "epoch 39; iter: 0; batch classifier loss: 0.348747; batch adversarial loss: 0.404896\n",
      "epoch 39; iter: 200; batch classifier loss: 0.306165; batch adversarial loss: 0.568043\n",
      "epoch 40; iter: 0; batch classifier loss: 0.292048; batch adversarial loss: 0.324407\n",
      "epoch 40; iter: 200; batch classifier loss: 0.392504; batch adversarial loss: 0.413571\n",
      "epoch 41; iter: 0; batch classifier loss: 0.403996; batch adversarial loss: 0.495722\n",
      "epoch 41; iter: 200; batch classifier loss: 0.384640; batch adversarial loss: 0.384941\n",
      "epoch 42; iter: 0; batch classifier loss: 0.400524; batch adversarial loss: 0.501964\n",
      "epoch 42; iter: 200; batch classifier loss: 0.345819; batch adversarial loss: 0.388777\n",
      "epoch 43; iter: 0; batch classifier loss: 0.389631; batch adversarial loss: 0.390537\n",
      "epoch 43; iter: 200; batch classifier loss: 0.478420; batch adversarial loss: 0.311937\n",
      "epoch 44; iter: 0; batch classifier loss: 0.526967; batch adversarial loss: 0.305604\n",
      "epoch 44; iter: 200; batch classifier loss: 0.325827; batch adversarial loss: 0.360945\n",
      "epoch 45; iter: 0; batch classifier loss: 0.322571; batch adversarial loss: 0.422701\n",
      "epoch 45; iter: 200; batch classifier loss: 0.443847; batch adversarial loss: 0.388873\n",
      "epoch 46; iter: 0; batch classifier loss: 0.284107; batch adversarial loss: 0.486327\n",
      "epoch 46; iter: 200; batch classifier loss: 0.506137; batch adversarial loss: 0.385864\n",
      "epoch 47; iter: 0; batch classifier loss: 0.413111; batch adversarial loss: 0.433013\n",
      "epoch 47; iter: 200; batch classifier loss: 0.367021; batch adversarial loss: 0.411750\n",
      "epoch 48; iter: 0; batch classifier loss: 0.438410; batch adversarial loss: 0.346707\n",
      "epoch 48; iter: 200; batch classifier loss: 0.340153; batch adversarial loss: 0.405444\n",
      "epoch 49; iter: 0; batch classifier loss: 0.392513; batch adversarial loss: 0.413926\n",
      "epoch 49; iter: 200; batch classifier loss: 0.435768; batch adversarial loss: 0.441039\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad_race = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"adult_debiasing_race\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_train_race, adult_test_race = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_ad_race.fit(adult_train_race)\n",
    "adult_predict_race = adult_ad_race.predict(adult_test_race)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd114b3-f52f-49ab-a471-8ca1040c452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.7997346502542935), 'Precision': np.float64(0.6807570977917982), 'Equal Opportunity Difference': np.float64(0.06123116278622465), 'Statistical Parity Difference': np.float64(-0.011656239262248)}\n"
     ]
    }
   ],
   "source": [
    "adult_race_metric_test = ClassificationMetric(\n",
    "    adult_test_race,\n",
    "    adult_predict_race,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    privileged_groups=adult_privileged_groups,\n",
    ")\n",
    "print({\n",
    "        \"Accuracy\": adult_race_metric_test.accuracy(),\n",
    "        \"Precision\":  adult_race_metric_test.precision(),\n",
    "        \"Equal Opportunity Difference\": adult_race_metric_test.equal_opportunity_difference(),\n",
    "        \"Statistical Parity Difference\": adult_race_metric_test.statistical_parity_difference(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41313c13-40c0-49cd-a7ae-da3cc47b1559",
   "metadata": {},
   "source": [
    "Adversary debiasing achieves better fairness performance when debiasing protected variable **race**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4475f-50e2-44cc-b8d5-88192363329c",
   "metadata": {},
   "source": [
    "# Update adversary_model with multiple inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e0b45-bb65-40e4-b850-a6a8b545bc52",
   "metadata": {},
   "source": [
    "The following return the inner layer `h1` from the two-layer neural network, and leverages the combinataion of inner layer and output layer of classifier_model for predicting protected attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6f0ae3-c13a-4728-a855-4d7b643c7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDebiasing(Transformer):\n",
    "    \"\"\"Adversarial debiasing is an in-processing technique that learns a\n",
    "    classifier to maximize prediction accuracy and simultaneously reduce an\n",
    "    adversary's ability to determine the protected attribute from the\n",
    "    predictions [5]_. This approach leads to a fair classifier as the\n",
    "    predictions cannot carry any group discrimination information that the\n",
    "    adversary can exploit.\n",
    "\n",
    "    References:\n",
    "        .. [5] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating Unwanted\n",
    "           Biases with Adversarial Learning,\" AAAI/ACM Conference on Artificial\n",
    "           Intelligence, Ethics, and Society, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unprivileged_groups,\n",
    "                 privileged_groups,\n",
    "                 scope_name,\n",
    "                 sess,\n",
    "                 seed=None,\n",
    "                 adversary_loss_weight=0.1,\n",
    "                 num_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 classifier_num_hidden_units=200,\n",
    "                 debias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (tuple): Representation for unprivileged groups\n",
    "            privileged_groups (tuple): Representation for privileged groups\n",
    "            scope_name (str): scope name for the tenforflow variables\n",
    "            sess (tf.Session): tensorflow session\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "            adversary_loss_weight (float, optional): Hyperparameter that chooses\n",
    "                the strength of the adversarial loss.\n",
    "            num_epochs (int, optional): Number of training epochs.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            classifier_num_hidden_units (int, optional): Number of hidden units\n",
    "                in the classifier model.\n",
    "            debias (bool, optional): Learn a classifier with or without\n",
    "                debiasing.\n",
    "        \"\"\"\n",
    "        super(AdversarialDebiasing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "        if len(self.unprivileged_groups) > 1 or len(self.privileged_groups) > 1:\n",
    "            raise ValueError(\"Only one unprivileged_group or privileged_group supported.\")\n",
    "        self.protected_attribute_name = list(self.unprivileged_groups[0].keys())[0]\n",
    "\n",
    "        self.sess = sess\n",
    "        self.adversary_loss_weight = adversary_loss_weight\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
    "        self.debias = debias\n",
    "\n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def _classifier_model(self, features, features_dim, keep_prob):\n",
    "        \"\"\"Compute the classifier predictions for the outcome variable.\n",
    "\n",
    "        Input\n",
    "        features: a tensor representing the input features for the classifier\n",
    "        features_dim: dimension of the feature dataset\n",
    "\n",
    "        Return\n",
    "        pred_label: predicted label\n",
    "        pred_logits: raw logits output from the output layer that holds\n",
    "        the unactivated score of prediction confidence\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"classifier_model\"):\n",
    "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
    "                                  initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
    "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
    "\n",
    "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_logit = tf.matmul(h1, W2) + b2\n",
    "            pred_label = tf.sigmoid(pred_logit) # predictive binary classification with sigmoid activation\n",
    "\n",
    "        return pred_label, pred_logit, h1\n",
    "\n",
    "    def _adversary_model(self, pred_logits, true_labels, hidden_layer):\n",
    "        \"\"\"Compute the adversary predictions for the protected attribute.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "            # Add hidden layer processing\n",
    "            W_hidden = tf.get_variable('W_hidden', [self.classifier_num_hidden_units, 10],\n",
    "                               initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            hidden_features = tf.matmul(hidden_layer, W_hidden)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [13, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "            combined_features = tf.concat([s, s * true_labels, s * (1.0 - true_labels), hidden_features], axis=1)\n",
    "\n",
    "            pred_protected_attribute_logit = tf.matmul(combined_features, W2) + b2\n",
    "            pred_protected_attribute_label = tf.sigmoid(pred_protected_attribute_logit)\n",
    "\n",
    "        return pred_protected_attribute_label, pred_protected_attribute_logit\n",
    "        \n",
    "    def evaluate_adversary(self, dataset):\n",
    "        \"\"\"Evaluate adversary's ability to predict protected attributes.\n",
    "        \n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing features and protected attributes\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_samples = len(dataset.features)\n",
    "        \n",
    "        # Initialize lists to store predictions and true values\n",
    "        pred_protected_attrs = []\n",
    "        true_protected_attrs = []\n",
    "        \n",
    "        # Predict in batches\n",
    "        for i in range(0, num_samples, self.batch_size):\n",
    "            batch_features = dataset.features[i:i + self.batch_size]\n",
    "            batch_labels = dataset.labels[i:i + self.batch_size].reshape(-1, 1)\n",
    "            batch_protected = dataset.protected_attributes[i:i + self.batch_size, \n",
    "                dataset.protected_attribute_names.index(self.protected_attribute_name)].reshape(-1, 1)\n",
    "            \n",
    "            # Create feed dictionary\n",
    "            feed_dict = {\n",
    "                self.features_ph: batch_features,\n",
    "                self.true_labels_ph: batch_labels,\n",
    "                self.keep_prob: 1.0\n",
    "            }\n",
    "            \n",
    "            # Get classifier predictions first\n",
    "            _, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            \n",
    "            # Get adversary predictions\n",
    "            pred_protected, _ = self._adversary_model(pred_logits, self.true_labels_ph)\n",
    "            \n",
    "            # Run session\n",
    "            pred_protected_batch = self.sess.run(pred_protected, feed_dict=feed_dict)\n",
    "            \n",
    "            # Store predictions and true values\n",
    "            pred_protected_attrs.extend((pred_protected_batch > 0.5).astype(int))\n",
    "            true_protected_attrs.extend(batch_protected)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        pred_protected_attrs = np.array(pred_protected_attrs)\n",
    "        true_protected_attrs = np.array(true_protected_attrs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(true_protected_attrs, pred_protected_attrs),\n",
    "            'auc': roc_auc_score(true_protected_attrs, pred_protected_attrs),\n",
    "            'confusion_matrix': confusion_matrix(true_protected_attrs, pred_protected_attrs)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Compute the model parameters of the fair classifier using gradient\n",
    "        descent.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "\n",
    "        Returns:\n",
    "            AdversarialDebiasing: Returns self.\n",
    "        \"\"\"\n",
    "        if tf.executing_eagerly():\n",
    "            raise RuntimeError(\"AdversarialDebiasing does not work in eager \"\n",
    "                    \"execution mode. To fix, add `tf.disable_eager_execution()`\"\n",
    "                    \" to the top of the calling script.\")\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        ii32 = np.iinfo(np.int32)\n",
    "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(ii32.min, ii32.max, size=4)\n",
    "\n",
    "        # Map the dataset labels to 0 and 1.\n",
    "        temp_labels = dataset.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(),0] = 1.0\n",
    "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(),0] = 0.0\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits, hidden_layer = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            # cross entropy loss between true and predicted labels\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(pred_logits, self.true_labels_ph, hidden_layer)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                       1000, 0.96, staircase=True)\n",
    "            # Tensorflow optimizer for classifier\n",
    "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                # Tensorflow optimizer for adversary\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            classifier_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'classifier_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'adversary_model' in var.name]\n",
    "                # Update classifier parameters\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n",
    "                                                                                      var_list=classifier_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "            classifier_grads = []\n",
    "            for (grad,var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                classifier_grads.append((grad, var))\n",
    "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                with tf.control_dependencies([classifier_minimizer]):\n",
    "                    adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars)#, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = dataset.features[batch_ids]\n",
    "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                                 dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                       self.true_labels_ph: batch_labels,\n",
    "                                       self.protected_attributes_ph: batch_protected_attributes,\n",
    "                                       self.keep_prob: 0.8}\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([classifier_minimizer,\n",
    "                                       adversary_minimizer,\n",
    "                                       pred_labels_loss,\n",
    "                                       pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run(\n",
    "                            [classifier_minimizer,\n",
    "                             pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair\n",
    "        classifier learned.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "        Returns:\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(dataset.features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = dataset.features[batch_ids]\n",
    "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                         dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy(deepcopy = True)\n",
    "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
    "        dataset_new.labels = (np.array(pred_labels)>0.5).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Map the dataset labels to back to their original values.\n",
    "        temp_labels = dataset_new.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
    "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
    "\n",
    "        dataset_new.labels = temp_labels.copy()\n",
    "\n",
    "        return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d2e02-be41-417a-b6ff-cd711f096ebf",
   "metadata": {},
   "source": [
    "## Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341faee9-89c3-4506-912c-f5f6b4aa0092",
   "metadata": {},
   "source": [
    "### Sensitive attribute = sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e1cb7c-84de-443b-8fb7-462c1d28b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Sandy/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Sandy/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733361640.150598 1144487 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 119.901993; batch adversarial loss: 35.531502\n",
      "epoch 0; iter: 200; batch classifier loss: 7.217835; batch adversarial loss: 4.258472\n",
      "epoch 1; iter: 0; batch classifier loss: 8.410116; batch adversarial loss: 7.039089\n",
      "epoch 1; iter: 200; batch classifier loss: 4.505497; batch adversarial loss: 9.530908\n",
      "epoch 2; iter: 0; batch classifier loss: 2.817708; batch adversarial loss: 4.587976\n",
      "epoch 2; iter: 200; batch classifier loss: 12.599235; batch adversarial loss: 10.079864\n",
      "epoch 3; iter: 0; batch classifier loss: 4.690023; batch adversarial loss: 2.500001\n",
      "epoch 3; iter: 200; batch classifier loss: 6.555622; batch adversarial loss: 6.149632\n",
      "epoch 4; iter: 0; batch classifier loss: 4.313360; batch adversarial loss: 1.807899\n",
      "epoch 4; iter: 200; batch classifier loss: 1.605585; batch adversarial loss: 2.160141\n",
      "epoch 5; iter: 0; batch classifier loss: 2.469866; batch adversarial loss: 1.280397\n",
      "epoch 5; iter: 200; batch classifier loss: 2.144440; batch adversarial loss: 0.649494\n",
      "epoch 6; iter: 0; batch classifier loss: 2.011220; batch adversarial loss: 0.974211\n",
      "epoch 6; iter: 200; batch classifier loss: 0.662744; batch adversarial loss: 1.883606\n",
      "epoch 7; iter: 0; batch classifier loss: 0.716801; batch adversarial loss: 1.233512\n",
      "epoch 7; iter: 200; batch classifier loss: 0.312906; batch adversarial loss: 0.687168\n",
      "epoch 8; iter: 0; batch classifier loss: 0.339023; batch adversarial loss: 0.712502\n",
      "epoch 8; iter: 200; batch classifier loss: 1.036674; batch adversarial loss: 0.651076\n",
      "epoch 9; iter: 0; batch classifier loss: 0.468473; batch adversarial loss: 0.844785\n",
      "epoch 9; iter: 200; batch classifier loss: 1.185296; batch adversarial loss: 0.602054\n",
      "epoch 10; iter: 0; batch classifier loss: 0.477437; batch adversarial loss: 0.672830\n",
      "epoch 10; iter: 200; batch classifier loss: 0.460576; batch adversarial loss: 0.592089\n",
      "epoch 11; iter: 0; batch classifier loss: 3.355370; batch adversarial loss: 4.194865\n",
      "epoch 11; iter: 200; batch classifier loss: 0.361880; batch adversarial loss: 1.111725\n",
      "epoch 12; iter: 0; batch classifier loss: 0.336943; batch adversarial loss: 1.812844\n",
      "epoch 12; iter: 200; batch classifier loss: 0.448751; batch adversarial loss: 0.594170\n",
      "epoch 13; iter: 0; batch classifier loss: 0.314816; batch adversarial loss: 0.574028\n",
      "epoch 13; iter: 200; batch classifier loss: 0.375808; batch adversarial loss: 0.617436\n",
      "epoch 14; iter: 0; batch classifier loss: 0.448789; batch adversarial loss: 0.571602\n",
      "epoch 14; iter: 200; batch classifier loss: 0.391843; batch adversarial loss: 3.190659\n",
      "epoch 15; iter: 0; batch classifier loss: 0.350475; batch adversarial loss: 0.579946\n",
      "epoch 15; iter: 200; batch classifier loss: 0.452940; batch adversarial loss: 0.671971\n",
      "epoch 16; iter: 0; batch classifier loss: 0.533001; batch adversarial loss: 1.192605\n",
      "epoch 16; iter: 200; batch classifier loss: 0.370840; batch adversarial loss: 0.605065\n",
      "epoch 17; iter: 0; batch classifier loss: 0.434744; batch adversarial loss: 0.605028\n",
      "epoch 17; iter: 200; batch classifier loss: 0.301418; batch adversarial loss: 0.562874\n",
      "epoch 18; iter: 0; batch classifier loss: 0.377340; batch adversarial loss: 0.568871\n",
      "epoch 18; iter: 200; batch classifier loss: 0.339663; batch adversarial loss: 0.828861\n",
      "epoch 19; iter: 0; batch classifier loss: 0.356833; batch adversarial loss: 0.691473\n",
      "epoch 19; iter: 200; batch classifier loss: 0.473845; batch adversarial loss: 0.593953\n",
      "epoch 20; iter: 0; batch classifier loss: 0.327005; batch adversarial loss: 0.723381\n",
      "epoch 20; iter: 200; batch classifier loss: 0.260204; batch adversarial loss: 0.730247\n",
      "epoch 21; iter: 0; batch classifier loss: 0.407781; batch adversarial loss: 0.708606\n",
      "epoch 21; iter: 200; batch classifier loss: 0.427669; batch adversarial loss: 0.621943\n",
      "epoch 22; iter: 0; batch classifier loss: 0.306661; batch adversarial loss: 0.578451\n",
      "epoch 22; iter: 200; batch classifier loss: 0.369936; batch adversarial loss: 1.279272\n",
      "epoch 23; iter: 0; batch classifier loss: 0.338087; batch adversarial loss: 0.570846\n",
      "epoch 23; iter: 200; batch classifier loss: 0.344966; batch adversarial loss: 0.691041\n",
      "epoch 24; iter: 0; batch classifier loss: 0.239292; batch adversarial loss: 1.130439\n",
      "epoch 24; iter: 200; batch classifier loss: 0.311284; batch adversarial loss: 0.606621\n",
      "epoch 25; iter: 0; batch classifier loss: 0.322103; batch adversarial loss: 0.846880\n",
      "epoch 25; iter: 200; batch classifier loss: 0.374018; batch adversarial loss: 0.800701\n",
      "epoch 26; iter: 0; batch classifier loss: 0.374631; batch adversarial loss: 0.683032\n",
      "epoch 26; iter: 200; batch classifier loss: 0.624431; batch adversarial loss: 0.606676\n",
      "epoch 27; iter: 0; batch classifier loss: 0.267673; batch adversarial loss: 0.762200\n",
      "epoch 27; iter: 200; batch classifier loss: 0.284744; batch adversarial loss: 0.608675\n",
      "epoch 28; iter: 0; batch classifier loss: 0.398976; batch adversarial loss: 0.594357\n",
      "epoch 28; iter: 200; batch classifier loss: 0.295474; batch adversarial loss: 0.618186\n",
      "epoch 29; iter: 0; batch classifier loss: 0.362720; batch adversarial loss: 0.770378\n",
      "epoch 29; iter: 200; batch classifier loss: 0.372265; batch adversarial loss: 0.545400\n",
      "epoch 30; iter: 0; batch classifier loss: 0.344617; batch adversarial loss: 0.624924\n",
      "epoch 30; iter: 200; batch classifier loss: 0.359877; batch adversarial loss: 0.585476\n",
      "epoch 31; iter: 0; batch classifier loss: 0.275693; batch adversarial loss: 0.670119\n",
      "epoch 31; iter: 200; batch classifier loss: 0.399511; batch adversarial loss: 0.639263\n",
      "epoch 32; iter: 0; batch classifier loss: 0.336631; batch adversarial loss: 0.581946\n",
      "epoch 32; iter: 200; batch classifier loss: 0.316483; batch adversarial loss: 0.664172\n",
      "epoch 33; iter: 0; batch classifier loss: 0.375257; batch adversarial loss: 0.639795\n",
      "epoch 33; iter: 200; batch classifier loss: 0.342648; batch adversarial loss: 0.636264\n",
      "epoch 34; iter: 0; batch classifier loss: 0.463757; batch adversarial loss: 0.647807\n",
      "epoch 34; iter: 200; batch classifier loss: 0.322852; batch adversarial loss: 0.602060\n",
      "epoch 35; iter: 0; batch classifier loss: 0.352357; batch adversarial loss: 0.621962\n",
      "epoch 35; iter: 200; batch classifier loss: 0.342481; batch adversarial loss: 0.624201\n",
      "epoch 36; iter: 0; batch classifier loss: 0.280692; batch adversarial loss: 0.613911\n",
      "epoch 36; iter: 200; batch classifier loss: 0.448889; batch adversarial loss: 0.567145\n",
      "epoch 37; iter: 0; batch classifier loss: 0.404990; batch adversarial loss: 0.570173\n",
      "epoch 37; iter: 200; batch classifier loss: 0.293064; batch adversarial loss: 0.638013\n",
      "epoch 38; iter: 0; batch classifier loss: 0.407829; batch adversarial loss: 0.546609\n",
      "epoch 38; iter: 200; batch classifier loss: 0.366905; batch adversarial loss: 0.610057\n",
      "epoch 39; iter: 0; batch classifier loss: 0.293333; batch adversarial loss: 0.607618\n",
      "epoch 39; iter: 200; batch classifier loss: 0.235145; batch adversarial loss: 0.643192\n",
      "epoch 40; iter: 0; batch classifier loss: 0.251209; batch adversarial loss: 0.681004\n",
      "epoch 40; iter: 200; batch classifier loss: 0.376443; batch adversarial loss: 0.556815\n",
      "epoch 41; iter: 0; batch classifier loss: 0.399587; batch adversarial loss: 0.564388\n",
      "epoch 41; iter: 200; batch classifier loss: 0.294796; batch adversarial loss: 0.607319\n",
      "epoch 42; iter: 0; batch classifier loss: 0.326062; batch adversarial loss: 0.568198\n",
      "epoch 42; iter: 200; batch classifier loss: 0.393523; batch adversarial loss: 0.624305\n",
      "epoch 43; iter: 0; batch classifier loss: 0.386369; batch adversarial loss: 0.583583\n",
      "epoch 43; iter: 200; batch classifier loss: 0.418488; batch adversarial loss: 0.618363\n",
      "epoch 44; iter: 0; batch classifier loss: 0.290009; batch adversarial loss: 0.583176\n",
      "epoch 44; iter: 200; batch classifier loss: 0.398850; batch adversarial loss: 0.627670\n",
      "epoch 45; iter: 0; batch classifier loss: 0.278133; batch adversarial loss: 0.699134\n",
      "epoch 45; iter: 200; batch classifier loss: 0.414902; batch adversarial loss: 0.633961\n",
      "epoch 46; iter: 0; batch classifier loss: 0.354342; batch adversarial loss: 0.606851\n",
      "epoch 46; iter: 200; batch classifier loss: 0.532953; batch adversarial loss: 0.598614\n",
      "epoch 47; iter: 0; batch classifier loss: 0.404184; batch adversarial loss: 0.624173\n",
      "epoch 47; iter: 200; batch classifier loss: 0.327457; batch adversarial loss: 0.589602\n",
      "epoch 48; iter: 0; batch classifier loss: 0.282992; batch adversarial loss: 0.570030\n",
      "epoch 48; iter: 200; batch classifier loss: 0.330170; batch adversarial loss: 0.587768\n",
      "epoch 49; iter: 0; batch classifier loss: 0.398692; batch adversarial loss: 0.664549\n",
      "epoch 49; iter: 200; batch classifier loss: 0.280388; batch adversarial loss: 0.628258\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import AdultDataset\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "adult_dataset = AdultDataset()\n",
    "adult_train, adult_test = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_privileged_groups = [{'sex': 1}]  # Male\n",
    "adult_unprivileged_groups = [{'sex': 0}]  # Female\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"adult_debiasing\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_ad.fit(adult_train)\n",
    "adult_predict = adult_ad.predict(adult_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1749a2-1d0b-49fe-af12-87b398f4fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/inFairness/utils/ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "/Users/Sandy/Library/Python/3.9/lib/python/site-packages/inFairness/utils/ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02262e7-1e8a-406a-83d2-f6b9fc24a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_metric_test = ClassificationMetric(\n",
    "    adult_test,\n",
    "    adult_predict,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    privileged_groups=adult_privileged_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e283462-a34a-4f89-9c43-ccb3f1c54012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.8452863566005749), 'Precision': np.float64(0.7516939019529693), 'Equal Opportunity Difference': np.float64(0.022709553296986873), 'Statistical Parity Difference': np.float64(-0.12879800558656207)}\n"
     ]
    }
   ],
   "source": [
    "print({\n",
    "        \"Accuracy\": adult_metric_test.accuracy(),\n",
    "        \"Precision\":  adult_metric_test.precision(),\n",
    "        \"Equal Opportunity Difference\": adult_metric_test.equal_opportunity_difference(),\n",
    "        \"Statistical Parity Difference\": adult_metric_test.statistical_parity_difference(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d059b3-d8e8-4378-adb0-3140bb1f677e",
   "metadata": {},
   "source": [
    "### sensitive attribute = race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c4a05b-2d64-4d1f-b860-ea1287e2a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 15.089951; batch adversarial loss: 109.427422\n",
      "epoch 0; iter: 200; batch classifier loss: 5.532632; batch adversarial loss: 6.326194\n",
      "epoch 1; iter: 0; batch classifier loss: 6.681258; batch adversarial loss: 2.966672\n",
      "epoch 1; iter: 200; batch classifier loss: 2.306801; batch adversarial loss: 2.686831\n",
      "epoch 2; iter: 0; batch classifier loss: 3.488862; batch adversarial loss: 3.756477\n",
      "epoch 2; iter: 200; batch classifier loss: 1.543002; batch adversarial loss: 1.238295\n",
      "epoch 3; iter: 0; batch classifier loss: 0.667877; batch adversarial loss: 20.976915\n",
      "epoch 3; iter: 200; batch classifier loss: 0.877956; batch adversarial loss: 0.717201\n",
      "epoch 4; iter: 0; batch classifier loss: 1.004907; batch adversarial loss: 1.572234\n",
      "epoch 4; iter: 200; batch classifier loss: 1.514891; batch adversarial loss: 2.660866\n",
      "epoch 5; iter: 0; batch classifier loss: 0.419609; batch adversarial loss: 1.168668\n",
      "epoch 5; iter: 200; batch classifier loss: 10.977070; batch adversarial loss: 3.589990\n",
      "epoch 6; iter: 0; batch classifier loss: 2.574787; batch adversarial loss: 3.653071\n",
      "epoch 6; iter: 200; batch classifier loss: 1.248596; batch adversarial loss: 1.514396\n",
      "epoch 7; iter: 0; batch classifier loss: 0.559019; batch adversarial loss: 9.533498\n",
      "epoch 7; iter: 200; batch classifier loss: 0.519460; batch adversarial loss: 1.087628\n",
      "epoch 8; iter: 0; batch classifier loss: 0.630363; batch adversarial loss: 20.519724\n",
      "epoch 8; iter: 200; batch classifier loss: 0.569407; batch adversarial loss: 0.486834\n",
      "epoch 9; iter: 0; batch classifier loss: 0.473166; batch adversarial loss: 1.544656\n",
      "epoch 9; iter: 200; batch classifier loss: 0.659601; batch adversarial loss: 4.530285\n",
      "epoch 10; iter: 0; batch classifier loss: 0.410615; batch adversarial loss: 0.473178\n",
      "epoch 10; iter: 200; batch classifier loss: 0.619256; batch adversarial loss: 0.502614\n",
      "epoch 11; iter: 0; batch classifier loss: 0.430435; batch adversarial loss: 0.666708\n",
      "epoch 11; iter: 200; batch classifier loss: 0.330963; batch adversarial loss: 1.633111\n",
      "epoch 12; iter: 0; batch classifier loss: 0.599863; batch adversarial loss: 3.613261\n",
      "epoch 12; iter: 200; batch classifier loss: 0.559538; batch adversarial loss: 0.414199\n",
      "epoch 13; iter: 0; batch classifier loss: 0.475115; batch adversarial loss: 5.664587\n",
      "epoch 13; iter: 200; batch classifier loss: 0.964832; batch adversarial loss: 0.885820\n",
      "epoch 14; iter: 0; batch classifier loss: 0.283063; batch adversarial loss: 0.393858\n",
      "epoch 14; iter: 200; batch classifier loss: 0.413311; batch adversarial loss: 1.041129\n",
      "epoch 15; iter: 0; batch classifier loss: 0.580699; batch adversarial loss: 0.479067\n",
      "epoch 15; iter: 200; batch classifier loss: 0.403753; batch adversarial loss: 3.017623\n",
      "epoch 16; iter: 0; batch classifier loss: 0.348782; batch adversarial loss: 0.784091\n",
      "epoch 16; iter: 200; batch classifier loss: 0.422586; batch adversarial loss: 3.209817\n",
      "epoch 17; iter: 0; batch classifier loss: 0.384967; batch adversarial loss: 0.395653\n",
      "epoch 17; iter: 200; batch classifier loss: 0.531967; batch adversarial loss: 0.643129\n",
      "epoch 18; iter: 0; batch classifier loss: 0.355713; batch adversarial loss: 1.055475\n",
      "epoch 18; iter: 200; batch classifier loss: 0.392470; batch adversarial loss: 2.048312\n",
      "epoch 19; iter: 0; batch classifier loss: 0.364922; batch adversarial loss: 0.998817\n",
      "epoch 19; iter: 200; batch classifier loss: 0.379709; batch adversarial loss: 0.698113\n",
      "epoch 20; iter: 0; batch classifier loss: 0.360584; batch adversarial loss: 0.514052\n",
      "epoch 20; iter: 200; batch classifier loss: 0.591127; batch adversarial loss: 1.165992\n",
      "epoch 21; iter: 0; batch classifier loss: 0.439208; batch adversarial loss: 0.478532\n",
      "epoch 21; iter: 200; batch classifier loss: 0.367977; batch adversarial loss: 1.177896\n",
      "epoch 22; iter: 0; batch classifier loss: 0.359058; batch adversarial loss: 0.622858\n",
      "epoch 22; iter: 200; batch classifier loss: 0.371297; batch adversarial loss: 1.236395\n",
      "epoch 23; iter: 0; batch classifier loss: 0.603401; batch adversarial loss: 0.552801\n",
      "epoch 23; iter: 200; batch classifier loss: 0.368260; batch adversarial loss: 1.207391\n",
      "epoch 24; iter: 0; batch classifier loss: 0.341467; batch adversarial loss: 0.407247\n",
      "epoch 24; iter: 200; batch classifier loss: 0.330332; batch adversarial loss: 1.323291\n",
      "epoch 25; iter: 0; batch classifier loss: 0.360930; batch adversarial loss: 0.491871\n",
      "epoch 25; iter: 200; batch classifier loss: 0.407904; batch adversarial loss: 1.096354\n",
      "epoch 26; iter: 0; batch classifier loss: 0.425390; batch adversarial loss: 0.893495\n",
      "epoch 26; iter: 200; batch classifier loss: 0.483003; batch adversarial loss: 0.336164\n",
      "epoch 27; iter: 0; batch classifier loss: 0.371062; batch adversarial loss: 0.395889\n",
      "epoch 27; iter: 200; batch classifier loss: 0.929677; batch adversarial loss: 0.468768\n",
      "epoch 28; iter: 0; batch classifier loss: 0.320364; batch adversarial loss: 0.382406\n",
      "epoch 28; iter: 200; batch classifier loss: 0.387144; batch adversarial loss: 0.325701\n",
      "epoch 29; iter: 0; batch classifier loss: 0.375313; batch adversarial loss: 0.641704\n",
      "epoch 29; iter: 200; batch classifier loss: 0.339990; batch adversarial loss: 0.748528\n",
      "epoch 30; iter: 0; batch classifier loss: 0.408315; batch adversarial loss: 0.416456\n",
      "epoch 30; iter: 200; batch classifier loss: 0.447707; batch adversarial loss: 0.444554\n",
      "epoch 31; iter: 0; batch classifier loss: 0.376934; batch adversarial loss: 0.352847\n",
      "epoch 31; iter: 200; batch classifier loss: 0.439285; batch adversarial loss: 0.416638\n",
      "epoch 32; iter: 0; batch classifier loss: 0.364427; batch adversarial loss: 3.850723\n",
      "epoch 32; iter: 200; batch classifier loss: 0.309190; batch adversarial loss: 0.553600\n",
      "epoch 33; iter: 0; batch classifier loss: 0.374778; batch adversarial loss: 0.897948\n",
      "epoch 33; iter: 200; batch classifier loss: 0.403080; batch adversarial loss: 0.622275\n",
      "epoch 34; iter: 0; batch classifier loss: 0.379749; batch adversarial loss: 0.530586\n",
      "epoch 34; iter: 200; batch classifier loss: 0.360771; batch adversarial loss: 5.560995\n",
      "epoch 35; iter: 0; batch classifier loss: 0.326410; batch adversarial loss: 0.405355\n",
      "epoch 35; iter: 200; batch classifier loss: 0.403299; batch adversarial loss: 0.652715\n",
      "epoch 36; iter: 0; batch classifier loss: 0.319074; batch adversarial loss: 0.435395\n",
      "epoch 36; iter: 200; batch classifier loss: 0.337582; batch adversarial loss: 0.461658\n",
      "epoch 37; iter: 0; batch classifier loss: 0.368333; batch adversarial loss: 0.477433\n",
      "epoch 37; iter: 200; batch classifier loss: 0.308459; batch adversarial loss: 0.444263\n",
      "epoch 38; iter: 0; batch classifier loss: 0.427569; batch adversarial loss: 0.392778\n",
      "epoch 38; iter: 200; batch classifier loss: 0.292703; batch adversarial loss: 0.559287\n",
      "epoch 39; iter: 0; batch classifier loss: 0.296154; batch adversarial loss: 0.374807\n",
      "epoch 39; iter: 200; batch classifier loss: 0.289203; batch adversarial loss: 0.813747\n",
      "epoch 40; iter: 0; batch classifier loss: 0.290846; batch adversarial loss: 0.424073\n",
      "epoch 40; iter: 200; batch classifier loss: 0.296528; batch adversarial loss: 0.352419\n",
      "epoch 41; iter: 0; batch classifier loss: 0.310613; batch adversarial loss: 4.622368\n",
      "epoch 41; iter: 200; batch classifier loss: 0.296431; batch adversarial loss: 0.630344\n",
      "epoch 42; iter: 0; batch classifier loss: 0.405861; batch adversarial loss: 0.771723\n",
      "epoch 42; iter: 200; batch classifier loss: 0.334025; batch adversarial loss: 1.079526\n",
      "epoch 43; iter: 0; batch classifier loss: 0.311588; batch adversarial loss: 0.604474\n",
      "epoch 43; iter: 200; batch classifier loss: 0.422020; batch adversarial loss: 0.589030\n",
      "epoch 44; iter: 0; batch classifier loss: 0.359903; batch adversarial loss: 1.560141\n",
      "epoch 44; iter: 200; batch classifier loss: 0.397084; batch adversarial loss: 0.702431\n",
      "epoch 45; iter: 0; batch classifier loss: 0.269156; batch adversarial loss: 0.446442\n",
      "epoch 45; iter: 200; batch classifier loss: 0.458567; batch adversarial loss: 0.525346\n",
      "epoch 46; iter: 0; batch classifier loss: 0.375718; batch adversarial loss: 0.435465\n",
      "epoch 46; iter: 200; batch classifier loss: 0.299455; batch adversarial loss: 0.404615\n",
      "epoch 47; iter: 0; batch classifier loss: 0.343918; batch adversarial loss: 0.530905\n",
      "epoch 47; iter: 200; batch classifier loss: 0.371260; batch adversarial loss: 0.415164\n",
      "epoch 48; iter: 0; batch classifier loss: 0.354298; batch adversarial loss: 0.363027\n",
      "epoch 48; iter: 200; batch classifier loss: 0.320211; batch adversarial loss: 0.475643\n",
      "epoch 49; iter: 0; batch classifier loss: 0.281165; batch adversarial loss: 0.429439\n",
      "epoch 49; iter: 200; batch classifier loss: 0.422650; batch adversarial loss: 0.435733\n"
     ]
    }
   ],
   "source": [
    "adult_privileged_groups = [{'race': 1}]  # white\n",
    "adult_unprivileged_groups = [{'race': 0}]  # non-white\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad_race = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"adult_debiasing_race\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_train_race, adult_test_race = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_ad_race.fit(adult_train_race)\n",
    "adult_predict_race = adult_ad_race.predict(adult_test_race)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68653127-1f83-4c95-85f3-01d15a25e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.8408638608388), 'Precision': np.float64(0.7297297297297297), 'Equal Opportunity Difference': np.float64(0.04281353712275093), 'Statistical Parity Difference': np.float64(-0.05788950717593025)}\n"
     ]
    }
   ],
   "source": [
    "adult_race_metric_test = ClassificationMetric(\n",
    "    adult_test_race,\n",
    "    adult_predict_race,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    privileged_groups=adult_privileged_groups,\n",
    ")\n",
    "print({\n",
    "        \"Accuracy\": adult_race_metric_test.accuracy(),\n",
    "        \"Precision\":  adult_race_metric_test.precision(),\n",
    "        \"Equal Opportunity Difference\": adult_race_metric_test.equal_opportunity_difference(),\n",
    "        \"Statistical Parity Difference\": adult_race_metric_test.statistical_parity_difference(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8fb3e-28fa-478a-a430-20a8a8621b8a",
   "metadata": {},
   "source": [
    "# Add adversary model evaluation impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f8f85-e9de-4c27-b7fe-7b0369db1801",
   "metadata": {},
   "source": [
    "`evaluate_adversary` helps evaluate `adversary_model`'s ability to predict sensitive attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338e84ea-6ec7-497f-89ac-b8e0877def1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDebiasing(Transformer):\n",
    "    \"\"\"Adversarial debiasing is an in-processing technique that learns a\n",
    "    classifier to maximize prediction accuracy and simultaneously reduce an\n",
    "    adversary's ability to determine the protected attribute from the\n",
    "    predictions [5]_. This approach leads to a fair classifier as the\n",
    "    predictions cannot carry any group discrimination information that the\n",
    "    adversary can exploit.\n",
    "\n",
    "    References:\n",
    "        .. [5] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating Unwanted\n",
    "           Biases with Adversarial Learning,\" AAAI/ACM Conference on Artificial\n",
    "           Intelligence, Ethics, and Society, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unprivileged_groups,\n",
    "                 privileged_groups,\n",
    "                 scope_name,\n",
    "                 sess,\n",
    "                 seed=None,\n",
    "                 adversary_loss_weight=0.1,\n",
    "                 num_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 classifier_num_hidden_units=200,\n",
    "                 debias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (tuple): Representation for unprivileged groups\n",
    "            privileged_groups (tuple): Representation for privileged groups\n",
    "            scope_name (str): scope name for the tenforflow variables\n",
    "            sess (tf.Session): tensorflow session\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "            adversary_loss_weight (float, optional): Hyperparameter that chooses\n",
    "                the strength of the adversarial loss.\n",
    "            num_epochs (int, optional): Number of training epochs.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            classifier_num_hidden_units (int, optional): Number of hidden units\n",
    "                in the classifier model.\n",
    "            debias (bool, optional): Learn a classifier with or without\n",
    "                debiasing.\n",
    "        \"\"\"\n",
    "        super(AdversarialDebiasing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "        if len(self.unprivileged_groups) > 1 or len(self.privileged_groups) > 1:\n",
    "            raise ValueError(\"Only one unprivileged_group or privileged_group supported.\")\n",
    "        self.protected_attribute_name = list(self.unprivileged_groups[0].keys())[0]\n",
    "\n",
    "        self.sess = sess\n",
    "        self.adversary_loss_weight = adversary_loss_weight\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
    "        self.debias = debias\n",
    "\n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "        self.pred_protected_attributes = None\n",
    "\n",
    "    def _classifier_model(self, features, features_dim, keep_prob):\n",
    "        \"\"\"Compute the classifier predictions for the outcome variable.\n",
    "\n",
    "        Input\n",
    "        features: a tensor representing the input features for the classifier\n",
    "        features_dim: dimension of the feature dataset\n",
    "\n",
    "        Return\n",
    "        pred_label: predicted label\n",
    "        pred_logits: raw logits output from the output layer that holds\n",
    "        the unactivated score of prediction confidence\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"classifier_model\"):\n",
    "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
    "                                  initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
    "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
    "\n",
    "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_logit = tf.matmul(h1, W2) + b2\n",
    "            pred_label = tf.sigmoid(pred_logit) # predictive binary classification with sigmoid activation\n",
    "\n",
    "        return pred_label, pred_logit, h1\n",
    "\n",
    "    def _adversary_model(self, pred_logits, true_labels, hidden_layer):\n",
    "        \"\"\"Compute the adversary predictions for the protected attribute.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "            # Add hidden layer processing\n",
    "            W_hidden = tf.get_variable('W_hidden', [self.classifier_num_hidden_units, 10],\n",
    "                               initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            hidden_features = tf.matmul(hidden_layer, W_hidden)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [13, 1],\n",
    "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "            combined_features = tf.concat([s, s * true_labels, s * (1.0 - true_labels), hidden_features], axis=1)\n",
    "\n",
    "            pred_protected_attribute_logit = tf.matmul(combined_features, W2) + b2\n",
    "            pred_protected_attribute_label = tf.sigmoid(pred_protected_attribute_logit)\n",
    "\n",
    "        return pred_protected_attribute_label, pred_protected_attribute_logit\n",
    "        \n",
    "    def evaluate_adversary(self, dataset):\n",
    "        \"\"\"Evaluate adversary's ability to predict protected attributes.\n",
    "        \n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing features and protected attributes\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_samples = len(dataset.features)\n",
    "        pred_protected_attrs = []\n",
    "        true_protected_attrs = []\n",
    "        \n",
    "        for i in range(0, num_samples, self.batch_size):\n",
    "            batch_features = dataset.features[i:i + self.batch_size]\n",
    "            batch_labels = dataset.labels[i:i + self.batch_size].reshape(-1, 1)\n",
    "            batch_protected = dataset.protected_attributes[i:i + self.batch_size, \n",
    "                dataset.protected_attribute_names.index(self.protected_attribute_name)].reshape(-1, 1)\n",
    "            \n",
    "            feed_dict = {\n",
    "                self.features_ph: batch_features,\n",
    "                self.true_labels_ph: batch_labels,\n",
    "                self.protected_attributes_ph: batch_protected,\n",
    "                self.keep_prob: 1.0\n",
    "            }\n",
    "            \n",
    "            pred_protected_batch = self.sess.run(self.pred_protected_attributes, feed_dict=feed_dict)\n",
    "            \n",
    "            pred_protected_attrs.extend((pred_protected_batch > 0.5).astype(int))\n",
    "            true_protected_attrs.extend(batch_protected)\n",
    "\n",
    "        pred_protected_attrs = np.array(pred_protected_attrs)\n",
    "        true_protected_attrs = np.array(true_protected_attrs)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(true_protected_attrs, pred_protected_attrs),\n",
    "            'auc': roc_auc_score(true_protected_attrs, pred_protected_attrs),\n",
    "            'confusion_matrix': confusion_matrix(true_protected_attrs, pred_protected_attrs)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Compute the model parameters of the fair classifier using gradient\n",
    "        descent.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "\n",
    "        Returns:\n",
    "            AdversarialDebiasing: Returns self.\n",
    "        \"\"\"\n",
    "        if tf.executing_eagerly():\n",
    "            raise RuntimeError(\"AdversarialDebiasing does not work in eager \"\n",
    "                    \"execution mode. To fix, add `tf.disable_eager_execution()`\"\n",
    "                    \" to the top of the calling script.\")\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        ii32 = np.iinfo(np.int32)\n",
    "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(ii32.min, ii32.max, size=4)\n",
    "\n",
    "        # Map the dataset labels to 0 and 1.\n",
    "        temp_labels = dataset.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(),0] = 1.0\n",
    "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(),0] = 0.0\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits, hidden_layer = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            # cross entropy loss between true and predicted labels\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(pred_logits, self.true_labels_ph, hidden_layer)\n",
    "                self.pred_protected_attributes = pred_protected_attributes_labels\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                       1000, 0.96, staircase=True)\n",
    "            # Tensorflow optimizer for classifier\n",
    "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                # Tensorflow optimizer for adversary\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            classifier_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'classifier_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'adversary_model' in var.name]\n",
    "                # Update classifier parameters\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n",
    "                                                                                      var_list=classifier_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "            classifier_grads = []\n",
    "            for (grad,var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                classifier_grads.append((grad, var))\n",
    "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                with tf.control_dependencies([classifier_minimizer]):\n",
    "                    adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars)#, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = dataset.features[batch_ids]\n",
    "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                                 dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                       self.true_labels_ph: batch_labels,\n",
    "                                       self.protected_attributes_ph: batch_protected_attributes,\n",
    "                                       self.keep_prob: 0.8}\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([classifier_minimizer,\n",
    "                                       adversary_minimizer,\n",
    "                                       pred_labels_loss,\n",
    "                                       pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run(\n",
    "                            [classifier_minimizer,\n",
    "                             pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair\n",
    "        classifier learned.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "        Returns:\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(dataset.features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = dataset.features[batch_ids]\n",
    "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                         dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy(deepcopy = True)\n",
    "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
    "        dataset_new.labels = (np.array(pred_labels)>0.5).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Map the dataset labels to back to their original values.\n",
    "        temp_labels = dataset_new.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
    "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
    "\n",
    "        dataset_new.labels = temp_labels.copy()\n",
    "\n",
    "        return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5a8ee-cbbf-45cf-8a14-736dc82d6243",
   "metadata": {},
   "source": [
    "## Evaluate adversary_model for protected attribute = sex case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79941ad1-ac67-48c5-a664-c7fa36be9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 56.425098; batch adversarial loss: 15.921197\n",
      "epoch 0; iter: 200; batch classifier loss: 3.644402; batch adversarial loss: 8.841759\n",
      "epoch 1; iter: 0; batch classifier loss: 4.655004; batch adversarial loss: 5.581833\n",
      "epoch 1; iter: 200; batch classifier loss: 3.012911; batch adversarial loss: 6.471652\n",
      "epoch 2; iter: 0; batch classifier loss: 2.563766; batch adversarial loss: 4.264257\n",
      "epoch 2; iter: 200; batch classifier loss: 0.291487; batch adversarial loss: 11.848980\n",
      "epoch 3; iter: 0; batch classifier loss: 4.885805; batch adversarial loss: 2.573878\n",
      "epoch 3; iter: 200; batch classifier loss: 0.851050; batch adversarial loss: 1.044282\n",
      "epoch 4; iter: 0; batch classifier loss: 1.965853; batch adversarial loss: 8.589492\n",
      "epoch 4; iter: 200; batch classifier loss: 2.457242; batch adversarial loss: 1.142101\n",
      "epoch 5; iter: 0; batch classifier loss: 2.085353; batch adversarial loss: 1.294117\n",
      "epoch 5; iter: 200; batch classifier loss: 1.567454; batch adversarial loss: 0.858428\n",
      "epoch 6; iter: 0; batch classifier loss: 1.710059; batch adversarial loss: 0.533393\n",
      "epoch 6; iter: 200; batch classifier loss: 0.648526; batch adversarial loss: 8.795206\n",
      "epoch 7; iter: 0; batch classifier loss: 0.415341; batch adversarial loss: 4.780837\n",
      "epoch 7; iter: 200; batch classifier loss: 0.825869; batch adversarial loss: 3.470335\n",
      "epoch 8; iter: 0; batch classifier loss: 0.702193; batch adversarial loss: 4.263542\n",
      "epoch 8; iter: 200; batch classifier loss: 1.269031; batch adversarial loss: 0.823844\n",
      "epoch 9; iter: 0; batch classifier loss: 0.952187; batch adversarial loss: 1.014411\n",
      "epoch 9; iter: 200; batch classifier loss: 0.530960; batch adversarial loss: 0.672734\n",
      "epoch 10; iter: 0; batch classifier loss: 0.851325; batch adversarial loss: 0.590697\n",
      "epoch 10; iter: 200; batch classifier loss: 0.363152; batch adversarial loss: 0.962640\n",
      "epoch 11; iter: 0; batch classifier loss: 0.263858; batch adversarial loss: 1.342568\n",
      "epoch 11; iter: 200; batch classifier loss: 1.121132; batch adversarial loss: 0.990025\n",
      "epoch 12; iter: 0; batch classifier loss: 0.417442; batch adversarial loss: 0.559951\n",
      "epoch 12; iter: 200; batch classifier loss: 0.366507; batch adversarial loss: 0.533086\n",
      "epoch 13; iter: 0; batch classifier loss: 0.459121; batch adversarial loss: 0.849516\n",
      "epoch 13; iter: 200; batch classifier loss: 0.405693; batch adversarial loss: 1.206663\n",
      "epoch 14; iter: 0; batch classifier loss: 0.284361; batch adversarial loss: 0.893248\n",
      "epoch 14; iter: 200; batch classifier loss: 0.541767; batch adversarial loss: 0.570502\n",
      "epoch 15; iter: 0; batch classifier loss: 0.337237; batch adversarial loss: 0.589239\n",
      "epoch 15; iter: 200; batch classifier loss: 0.395990; batch adversarial loss: 1.129456\n",
      "epoch 16; iter: 0; batch classifier loss: 0.581094; batch adversarial loss: 1.239907\n",
      "epoch 16; iter: 200; batch classifier loss: 0.370518; batch adversarial loss: 0.624892\n",
      "epoch 17; iter: 0; batch classifier loss: 0.352287; batch adversarial loss: 1.441418\n",
      "epoch 17; iter: 200; batch classifier loss: 0.375024; batch adversarial loss: 0.697645\n",
      "epoch 18; iter: 0; batch classifier loss: 0.441238; batch adversarial loss: 0.696201\n",
      "epoch 18; iter: 200; batch classifier loss: 0.378710; batch adversarial loss: 1.456594\n",
      "epoch 19; iter: 0; batch classifier loss: 0.383888; batch adversarial loss: 0.623703\n",
      "epoch 19; iter: 200; batch classifier loss: 0.352737; batch adversarial loss: 0.962667\n",
      "epoch 20; iter: 0; batch classifier loss: 0.439180; batch adversarial loss: 0.916407\n",
      "epoch 20; iter: 200; batch classifier loss: 0.375455; batch adversarial loss: 0.767582\n",
      "epoch 21; iter: 0; batch classifier loss: 0.394392; batch adversarial loss: 0.557572\n",
      "epoch 21; iter: 200; batch classifier loss: 0.373843; batch adversarial loss: 0.590514\n",
      "epoch 22; iter: 0; batch classifier loss: 0.405967; batch adversarial loss: 0.780652\n",
      "epoch 22; iter: 200; batch classifier loss: 0.374414; batch adversarial loss: 1.780084\n",
      "epoch 23; iter: 0; batch classifier loss: 0.297439; batch adversarial loss: 0.548337\n",
      "epoch 23; iter: 200; batch classifier loss: 0.433082; batch adversarial loss: 0.718944\n",
      "epoch 24; iter: 0; batch classifier loss: 0.353137; batch adversarial loss: 2.156216\n",
      "epoch 24; iter: 200; batch classifier loss: 0.264545; batch adversarial loss: 4.294599\n",
      "epoch 25; iter: 0; batch classifier loss: 0.294943; batch adversarial loss: 1.138479\n",
      "epoch 25; iter: 200; batch classifier loss: 0.341908; batch adversarial loss: 0.592744\n",
      "epoch 26; iter: 0; batch classifier loss: 0.349405; batch adversarial loss: 0.903074\n",
      "epoch 26; iter: 200; batch classifier loss: 0.631660; batch adversarial loss: 0.661807\n",
      "epoch 27; iter: 0; batch classifier loss: 0.381287; batch adversarial loss: 0.664814\n",
      "epoch 27; iter: 200; batch classifier loss: 0.463444; batch adversarial loss: 1.016312\n",
      "epoch 28; iter: 0; batch classifier loss: 0.343107; batch adversarial loss: 0.692207\n",
      "epoch 28; iter: 200; batch classifier loss: 0.364951; batch adversarial loss: 0.582618\n",
      "epoch 29; iter: 0; batch classifier loss: 0.350852; batch adversarial loss: 0.668888\n",
      "epoch 29; iter: 200; batch classifier loss: 0.304304; batch adversarial loss: 1.143695\n",
      "epoch 30; iter: 0; batch classifier loss: 0.359570; batch adversarial loss: 0.955973\n",
      "epoch 30; iter: 200; batch classifier loss: 0.355600; batch adversarial loss: 0.554211\n",
      "epoch 31; iter: 0; batch classifier loss: 0.765208; batch adversarial loss: 0.942630\n",
      "epoch 31; iter: 200; batch classifier loss: 0.329442; batch adversarial loss: 3.615890\n",
      "epoch 32; iter: 0; batch classifier loss: 0.421328; batch adversarial loss: 0.840137\n",
      "epoch 32; iter: 200; batch classifier loss: 0.432676; batch adversarial loss: 0.668109\n",
      "epoch 33; iter: 0; batch classifier loss: 0.284099; batch adversarial loss: 0.603218\n",
      "epoch 33; iter: 200; batch classifier loss: 0.349379; batch adversarial loss: 0.573427\n",
      "epoch 34; iter: 0; batch classifier loss: 0.567674; batch adversarial loss: 0.634943\n",
      "epoch 34; iter: 200; batch classifier loss: 0.490185; batch adversarial loss: 0.620581\n",
      "epoch 35; iter: 0; batch classifier loss: 0.345882; batch adversarial loss: 0.752851\n",
      "epoch 35; iter: 200; batch classifier loss: 0.333343; batch adversarial loss: 0.802579\n",
      "epoch 36; iter: 0; batch classifier loss: 0.338299; batch adversarial loss: 0.606254\n",
      "epoch 36; iter: 200; batch classifier loss: 0.352903; batch adversarial loss: 0.554562\n",
      "epoch 37; iter: 0; batch classifier loss: 0.435735; batch adversarial loss: 0.877906\n",
      "epoch 37; iter: 200; batch classifier loss: 0.347146; batch adversarial loss: 0.650204\n",
      "epoch 38; iter: 0; batch classifier loss: 0.566997; batch adversarial loss: 0.635458\n",
      "epoch 38; iter: 200; batch classifier loss: 0.373776; batch adversarial loss: 0.641297\n",
      "epoch 39; iter: 0; batch classifier loss: 0.334081; batch adversarial loss: 0.742832\n",
      "epoch 39; iter: 200; batch classifier loss: 0.288904; batch adversarial loss: 0.678978\n",
      "epoch 40; iter: 0; batch classifier loss: 0.320910; batch adversarial loss: 0.752565\n",
      "epoch 40; iter: 200; batch classifier loss: 0.354327; batch adversarial loss: 0.646528\n",
      "epoch 41; iter: 0; batch classifier loss: 0.504108; batch adversarial loss: 0.804547\n",
      "epoch 41; iter: 200; batch classifier loss: 0.324584; batch adversarial loss: 0.861121\n",
      "epoch 42; iter: 0; batch classifier loss: 0.260264; batch adversarial loss: 1.551704\n",
      "epoch 42; iter: 200; batch classifier loss: 0.384962; batch adversarial loss: 0.636988\n",
      "epoch 43; iter: 0; batch classifier loss: 0.303317; batch adversarial loss: 0.651973\n",
      "epoch 43; iter: 200; batch classifier loss: 0.403299; batch adversarial loss: 0.691068\n",
      "epoch 44; iter: 0; batch classifier loss: 0.359666; batch adversarial loss: 0.582435\n",
      "epoch 44; iter: 200; batch classifier loss: 0.438965; batch adversarial loss: 1.052196\n",
      "epoch 45; iter: 0; batch classifier loss: 0.355200; batch adversarial loss: 1.533329\n",
      "epoch 45; iter: 200; batch classifier loss: 0.503692; batch adversarial loss: 0.555163\n",
      "epoch 46; iter: 0; batch classifier loss: 0.295809; batch adversarial loss: 0.687207\n",
      "epoch 46; iter: 200; batch classifier loss: 0.409657; batch adversarial loss: 0.666259\n",
      "epoch 47; iter: 0; batch classifier loss: 0.467378; batch adversarial loss: 0.817199\n",
      "epoch 47; iter: 200; batch classifier loss: 0.398430; batch adversarial loss: 0.743531\n",
      "epoch 48; iter: 0; batch classifier loss: 0.482606; batch adversarial loss: 0.694498\n",
      "epoch 48; iter: 200; batch classifier loss: 0.347294; batch adversarial loss: 0.690289\n",
      "epoch 49; iter: 0; batch classifier loss: 0.390895; batch adversarial loss: 0.588703\n",
      "epoch 49; iter: 200; batch classifier loss: 0.450267; batch adversarial loss: 1.214720\n"
     ]
    }
   ],
   "source": [
    "adult_dataset = AdultDataset()\n",
    "adult_train, adult_test = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_privileged_groups = [{'sex': 1}]  # Male\n",
    "adult_unprivileged_groups = [{'sex': 0}]  # Female\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"ad_eval_sex\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_ad.fit(adult_train)\n",
    "adult_predict = adult_ad.predict(adult_test)\n",
    "adversary_model_eval = adult_ad.evaluate_adversary(adult_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c7d36f9-5b39-4c3e-9a13-37c25c806d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6677231517653129,\n",
       " 'auc': np.float64(0.5015695604493574),\n",
       " 'confusion_matrix': array([[ 110, 4309],\n",
       "        [ 199, 8949]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary_model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da32bfa-d1c2-48fa-9328-113d93510af2",
   "metadata": {},
   "source": [
    "## Evaluate adversary_model for protected attribute = race case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81c4b6be-ca00-4be7-aa68-b76a36972187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 172.117157; batch adversarial loss: 74.262733\n",
      "epoch 0; iter: 200; batch classifier loss: 7.617409; batch adversarial loss: 9.049286\n",
      "epoch 1; iter: 0; batch classifier loss: 6.327208; batch adversarial loss: 3.348848\n",
      "epoch 1; iter: 200; batch classifier loss: 7.633907; batch adversarial loss: 9.172488\n",
      "epoch 2; iter: 0; batch classifier loss: 2.654064; batch adversarial loss: 0.427413\n",
      "epoch 2; iter: 200; batch classifier loss: 3.535689; batch adversarial loss: 1.480289\n",
      "epoch 3; iter: 0; batch classifier loss: 7.298629; batch adversarial loss: 2.112755\n",
      "epoch 3; iter: 200; batch classifier loss: 1.964802; batch adversarial loss: 0.659369\n",
      "epoch 4; iter: 0; batch classifier loss: 2.002451; batch adversarial loss: 0.830459\n",
      "epoch 4; iter: 200; batch classifier loss: 2.081960; batch adversarial loss: 11.801059\n",
      "epoch 5; iter: 0; batch classifier loss: 1.811082; batch adversarial loss: 1.593162\n",
      "epoch 5; iter: 200; batch classifier loss: 2.418119; batch adversarial loss: 1.417439\n",
      "epoch 6; iter: 0; batch classifier loss: 0.293248; batch adversarial loss: 0.933604\n",
      "epoch 6; iter: 200; batch classifier loss: 0.541653; batch adversarial loss: 0.447747\n",
      "epoch 7; iter: 0; batch classifier loss: 0.415663; batch adversarial loss: 0.437241\n",
      "epoch 7; iter: 200; batch classifier loss: 1.113377; batch adversarial loss: 0.471931\n",
      "epoch 8; iter: 0; batch classifier loss: 0.414773; batch adversarial loss: 0.412361\n",
      "epoch 8; iter: 200; batch classifier loss: 0.750030; batch adversarial loss: 0.520874\n",
      "epoch 9; iter: 0; batch classifier loss: 0.467639; batch adversarial loss: 0.505449\n",
      "epoch 9; iter: 200; batch classifier loss: 0.888151; batch adversarial loss: 1.218923\n",
      "epoch 10; iter: 0; batch classifier loss: 0.308951; batch adversarial loss: 0.522093\n",
      "epoch 10; iter: 200; batch classifier loss: 0.483162; batch adversarial loss: 0.431758\n",
      "epoch 11; iter: 0; batch classifier loss: 0.311032; batch adversarial loss: 0.433501\n",
      "epoch 11; iter: 200; batch classifier loss: 0.377092; batch adversarial loss: 0.434903\n",
      "epoch 12; iter: 0; batch classifier loss: 0.358411; batch adversarial loss: 0.316423\n",
      "epoch 12; iter: 200; batch classifier loss: 0.378333; batch adversarial loss: 0.432410\n",
      "epoch 13; iter: 0; batch classifier loss: 0.270217; batch adversarial loss: 1.170904\n",
      "epoch 13; iter: 200; batch classifier loss: 0.362930; batch adversarial loss: 0.469218\n",
      "epoch 14; iter: 0; batch classifier loss: 0.337026; batch adversarial loss: 0.607392\n",
      "epoch 14; iter: 200; batch classifier loss: 0.501805; batch adversarial loss: 0.478981\n",
      "epoch 15; iter: 0; batch classifier loss: 0.596355; batch adversarial loss: 0.644017\n",
      "epoch 15; iter: 200; batch classifier loss: 0.286593; batch adversarial loss: 0.371585\n",
      "epoch 16; iter: 0; batch classifier loss: 0.429150; batch adversarial loss: 0.443156\n",
      "epoch 16; iter: 200; batch classifier loss: 0.262242; batch adversarial loss: 0.443239\n",
      "epoch 17; iter: 0; batch classifier loss: 0.335710; batch adversarial loss: 0.435916\n",
      "epoch 17; iter: 200; batch classifier loss: 1.207651; batch adversarial loss: 1.132071\n",
      "epoch 18; iter: 0; batch classifier loss: 0.265270; batch adversarial loss: 0.420860\n",
      "epoch 18; iter: 200; batch classifier loss: 0.438636; batch adversarial loss: 0.483108\n",
      "epoch 19; iter: 0; batch classifier loss: 0.363022; batch adversarial loss: 0.416534\n",
      "epoch 19; iter: 200; batch classifier loss: 0.314745; batch adversarial loss: 0.344971\n",
      "epoch 20; iter: 0; batch classifier loss: 0.414421; batch adversarial loss: 0.387980\n",
      "epoch 20; iter: 200; batch classifier loss: 0.376642; batch adversarial loss: 0.567458\n",
      "epoch 21; iter: 0; batch classifier loss: 0.284651; batch adversarial loss: 0.373932\n",
      "epoch 21; iter: 200; batch classifier loss: 0.305684; batch adversarial loss: 0.391700\n",
      "epoch 22; iter: 0; batch classifier loss: 0.297750; batch adversarial loss: 0.316164\n",
      "epoch 22; iter: 200; batch classifier loss: 0.320006; batch adversarial loss: 0.571590\n",
      "epoch 23; iter: 0; batch classifier loss: 0.353999; batch adversarial loss: 0.343699\n",
      "epoch 23; iter: 200; batch classifier loss: 0.346293; batch adversarial loss: 0.500204\n",
      "epoch 24; iter: 0; batch classifier loss: 0.328621; batch adversarial loss: 0.344714\n",
      "epoch 24; iter: 200; batch classifier loss: 0.304449; batch adversarial loss: 0.439682\n",
      "epoch 25; iter: 0; batch classifier loss: 0.273831; batch adversarial loss: 0.394564\n",
      "epoch 25; iter: 200; batch classifier loss: 0.358696; batch adversarial loss: 0.439133\n",
      "epoch 26; iter: 0; batch classifier loss: 0.301942; batch adversarial loss: 0.310923\n",
      "epoch 26; iter: 200; batch classifier loss: 0.303814; batch adversarial loss: 0.747448\n",
      "epoch 27; iter: 0; batch classifier loss: 0.355106; batch adversarial loss: 0.400337\n",
      "epoch 27; iter: 200; batch classifier loss: 0.402829; batch adversarial loss: 0.398669\n",
      "epoch 28; iter: 0; batch classifier loss: 0.546673; batch adversarial loss: 0.401374\n",
      "epoch 28; iter: 200; batch classifier loss: 0.297201; batch adversarial loss: 0.493499\n",
      "epoch 29; iter: 0; batch classifier loss: 0.299225; batch adversarial loss: 1.123619\n",
      "epoch 29; iter: 200; batch classifier loss: 0.274238; batch adversarial loss: 0.422284\n",
      "epoch 30; iter: 0; batch classifier loss: 0.329973; batch adversarial loss: 0.405216\n",
      "epoch 30; iter: 200; batch classifier loss: 0.439597; batch adversarial loss: 0.419326\n",
      "epoch 31; iter: 0; batch classifier loss: 0.399927; batch adversarial loss: 0.393571\n",
      "epoch 31; iter: 200; batch classifier loss: 0.234701; batch adversarial loss: 0.355204\n",
      "epoch 32; iter: 0; batch classifier loss: 0.266826; batch adversarial loss: 0.455910\n",
      "epoch 32; iter: 200; batch classifier loss: 0.358226; batch adversarial loss: 0.381925\n",
      "epoch 33; iter: 0; batch classifier loss: 0.340667; batch adversarial loss: 0.372014\n",
      "epoch 33; iter: 200; batch classifier loss: 0.304776; batch adversarial loss: 0.392986\n",
      "epoch 34; iter: 0; batch classifier loss: 0.373257; batch adversarial loss: 0.398192\n",
      "epoch 34; iter: 200; batch classifier loss: 0.367922; batch adversarial loss: 0.522378\n",
      "epoch 35; iter: 0; batch classifier loss: 0.318883; batch adversarial loss: 0.364930\n",
      "epoch 35; iter: 200; batch classifier loss: 0.323752; batch adversarial loss: 0.400382\n",
      "epoch 36; iter: 0; batch classifier loss: 0.346386; batch adversarial loss: 0.336093\n",
      "epoch 36; iter: 200; batch classifier loss: 0.362647; batch adversarial loss: 0.387694\n",
      "epoch 37; iter: 0; batch classifier loss: 0.349719; batch adversarial loss: 0.419749\n",
      "epoch 37; iter: 200; batch classifier loss: 0.413037; batch adversarial loss: 0.508313\n",
      "epoch 38; iter: 0; batch classifier loss: 0.318874; batch adversarial loss: 0.355358\n",
      "epoch 38; iter: 200; batch classifier loss: 0.332416; batch adversarial loss: 0.433914\n",
      "epoch 39; iter: 0; batch classifier loss: 0.348141; batch adversarial loss: 0.306194\n",
      "epoch 39; iter: 200; batch classifier loss: 0.296022; batch adversarial loss: 0.412822\n",
      "epoch 40; iter: 0; batch classifier loss: 0.333328; batch adversarial loss: 0.710004\n",
      "epoch 40; iter: 200; batch classifier loss: 0.359212; batch adversarial loss: 0.422669\n",
      "epoch 41; iter: 0; batch classifier loss: 0.344984; batch adversarial loss: 0.513323\n",
      "epoch 41; iter: 200; batch classifier loss: 0.224600; batch adversarial loss: 0.363356\n",
      "epoch 42; iter: 0; batch classifier loss: 0.338870; batch adversarial loss: 0.472680\n",
      "epoch 42; iter: 200; batch classifier loss: 0.305742; batch adversarial loss: 0.488586\n",
      "epoch 43; iter: 0; batch classifier loss: 0.258723; batch adversarial loss: 0.318345\n",
      "epoch 43; iter: 200; batch classifier loss: 0.312504; batch adversarial loss: 0.567457\n",
      "epoch 44; iter: 0; batch classifier loss: 0.345699; batch adversarial loss: 0.350119\n",
      "epoch 44; iter: 200; batch classifier loss: 0.389094; batch adversarial loss: 0.342895\n",
      "epoch 45; iter: 0; batch classifier loss: 0.351887; batch adversarial loss: 0.453305\n",
      "epoch 45; iter: 200; batch classifier loss: 0.408779; batch adversarial loss: 0.427296\n",
      "epoch 46; iter: 0; batch classifier loss: 0.351469; batch adversarial loss: 0.443960\n",
      "epoch 46; iter: 200; batch classifier loss: 0.343440; batch adversarial loss: 0.328624\n",
      "epoch 47; iter: 0; batch classifier loss: 0.317552; batch adversarial loss: 0.328022\n",
      "epoch 47; iter: 200; batch classifier loss: 0.309869; batch adversarial loss: 0.369478\n",
      "epoch 48; iter: 0; batch classifier loss: 0.292854; batch adversarial loss: 0.381422\n",
      "epoch 48; iter: 200; batch classifier loss: 0.290111; batch adversarial loss: 0.517183\n",
      "epoch 49; iter: 0; batch classifier loss: 0.248004; batch adversarial loss: 0.848401\n",
      "epoch 49; iter: 200; batch classifier loss: 0.316245; batch adversarial loss: 0.369075\n"
     ]
    }
   ],
   "source": [
    "adult_privileged_groups = [{'race': 1}]  # white\n",
    "adult_unprivileged_groups = [{'race': 0}]  # non-white\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "adult_ad_race = AdversarialDebiasing(\n",
    "    privileged_groups=adult_privileged_groups,\n",
    "    unprivileged_groups=adult_unprivileged_groups,\n",
    "    scope_name=\"ad_eval_race\",\n",
    "    sess=sess,\n",
    ")\n",
    "adult_train_race, adult_test_race = adult_dataset.split([0.7], shuffle=True)\n",
    "adult_ad_race.fit(adult_train_race)\n",
    "adversary_model_eval_race = adult_ad_race.evaluate_adversary(adult_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e69b119a-bb83-4d9e-9b2e-3b09ddfd7d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8670302940959682,\n",
       " 'auc': np.float64(0.5),\n",
       " 'confusion_matrix': array([[    0,  1804],\n",
       "        [    0, 11763]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary_model_eval_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3059c48-84d3-48a6-9e40-efa4e325cde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
